{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Tanzu Gym \u00b6 This web site is here for documenting my own experience and learnings around the VMware Tanzu portfolio. It's not meant to be a comprehensive source of information, but it's more a reference for the future me, in order not to forget what I've learnt along my Tanzu journey. However, I'll be more than happy if it will help others, too. If you feel like contributing or improving the repo with additional docs, fixes and suggestions, feel free to create an issue .","title":"HOME"},{"location":"#the-tanzu-gym","text":"This web site is here for documenting my own experience and learnings around the VMware Tanzu portfolio. It's not meant to be a comprehensive source of information, but it's more a reference for the future me, in order not to forget what I've learnt along my Tanzu journey. However, I'll be more than happy if it will help others, too. If you feel like contributing or improving the repo with additional docs, fixes and suggestions, feel free to create an issue .","title":"The Tanzu Gym"},{"location":"tkgm/","text":"Preface \u00b6 The environment I'm testing TKG on is composed of a vSphere infrastructure (ESXi + vCenter), deployed along with a NSX-T platform to provide dynamic network segmentation and management. NSX-T is not strictly mandatory to follow along, it's just convenient for me for setting up the network segments, routers and policies to my own liking without the need for physical devices, as well as the ability to configure everything via API, that makes me able to provision an entire stack automatically within minutes. This section is about deploying and managing a Tanzu Kubernetes Grid 2.1 platform with a Standalone Management Cluster . What this guide is NOT meant for deploying vSphere with Tanzu giving comprehensive training around Kubernetes deep-diving into the tools used (i.e. Linux, bash, govc, Terraform, ...) However, some snippets are provided as a reference that you will need to adjust to your own environment, whilst some others are even ready to be used OOTB. Feedbacks are welcome If you think that something is not clear enough or totally wrong, please do create an issue and let me know.","title":"Preface"},{"location":"tkgm/#preface","text":"The environment I'm testing TKG on is composed of a vSphere infrastructure (ESXi + vCenter), deployed along with a NSX-T platform to provide dynamic network segmentation and management. NSX-T is not strictly mandatory to follow along, it's just convenient for me for setting up the network segments, routers and policies to my own liking without the need for physical devices, as well as the ability to configure everything via API, that makes me able to provision an entire stack automatically within minutes. This section is about deploying and managing a Tanzu Kubernetes Grid 2.1 platform with a Standalone Management Cluster . What this guide is NOT meant for deploying vSphere with Tanzu giving comprehensive training around Kubernetes deep-diving into the tools used (i.e. Linux, bash, govc, Terraform, ...) However, some snippets are provided as a reference that you will need to adjust to your own environment, whilst some others are even ready to be used OOTB. Feedbacks are welcome If you think that something is not clear enough or totally wrong, please do create an issue and let me know.","title":"Preface"},{"location":"tkgm/bastion/","text":"Prepare bastion host \u00b6 You will need a Linux bastion host equipped with a few tools to be able to fully deploy and operate TKGM. You can pick any distribution of choice, as long as you know how to do a basic configuration and deploy software on it. However, in this guide, I provide guidance on Ubuntu and RHEL-like (i.e. RHEL, CentOS, Oracle Linux) distributions only. It is also recommended not to use the root user, but instead create an unprivileged user (belonging to wheel group) and use it for day1 and day2 operations. In this guide I am using the tanzu user, that can be created as useradd -m -G wheel tanzu To allow the bastion host to relocate TKGM images , the minimum requirements for the bastion host are 2GB RAM, 2 vCPU and 45 GB free hard disk space, therefore it's better to provide at least ~10GB more for OS and packages installation. If you have fine-grained file systems, make sure you grant at least 20 GB free disk space for the /tmp directory, for storing temporary files during images relocation. Proxy configuration (optional) \u00b6 NSX-T can be used to create segregated networks with proper policies to block egress traffic, and simulate an air-gapped environment for the TKGM cluster The bastion host is used to fetch the required images and copy them to the TKGM isolated network, therefore it somehow needs to access the Internet, i.e. via a proxy. For simulating a proxy we can use another host running a basic Squid installation. On Ubuntu, it can be installed as apt install -y squid By default, Squid denies all outgoing connections if not explicitly authorised via a http_access directive, like acl external_allowed dstdomain \"/etc/squid/conf.d/allowed-domains.txt\" http_access allow external_allowed Software installation \u00b6 First of all, you need to identify a directory where you want to store those binaries. In the following guide, we'll be using the LOCALBIN variable you can set to whatever directory you are comfortable with, but make sure you also add it to your user's PATH environment variable. Also, if writing to it requires superuser privileges, do not forget to prepend every write command with sudo . LOCALBIN = \" $HOME /.local/bin\" mkdir -p $LOCALBIN export PATH = $PATH : $LOCALBIN OS tools \u00b6 Oracle Linux sudo dnf install -y bash-completion bind-utils net-tools traceroute git \u00b6 Most distributions come with git pre-installed, but in case yours doesn't you can leverage your package manager to get it: Oracle Linux Ubuntu sudo dnf install -y git-all sudo apt install -y git kubectl \u00b6 The latest kubectl release can be installed as curl -sSfLO \"https://dl.k8s.io/release/ $( curl -sSfL https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" install kubectl ${ LOCALBIN } /kubectl kubectx and kubens \u00b6 Helper scripts to easily switch kubeconfig context and set default namespace. You may want to get the scripts from an actual release and not from the main branch. LATEST = \"v0.9.4\" sudo git clone --depth 1 --branch $LATEST -c advice.detachedHead = false https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -sf /opt/kubectx/kubectx ${ LOCALBIN } /kubectx sudo ln -sf /opt/kubectx/kubens ${ LOCALBIN } /kubens helm \u00b6 curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | HELM_INSTALL_DIR = ~/.local/bin bash fzf \u00b6 fzf is a general-purpose command-line fuzzy finder. curl -sSfL https://github.com/junegunn/fzf/releases/download/0.38.0/fzf-0.38.0-linux_amd64.tar.gz | tar xzf - install fzf ${ LOCALBIN } /fzf yq \u00b6 curl -sSfL -o yq https://github.com/mikefarah/yq/releases/download/v4.30.8/yq_linux_amd64 install yq ${ LOCALBIN } /yq jq \u00b6 Oracle Linux Ubuntu sudo dnf install -y jq sudo apt install -y jq docker \u00b6 Oracle Linux sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo dnf update -y sudo dnf install -y docker-ce docker-ce-cli containerd.io Start and enable the docker service: sudo systemctl start docker sudo systemctl enable docker Finally, add the user to the docker group: sudo gpasswd -a tanzu docker vcc \u00b6 vcc is a useful CLI tool for downloading packages from customerconnect.vmware.com. Get the latest release from https://github.com/vmware-labs/vmware-customer-connect-cli/releases , i.e.: curl -sSfL -o vcc https://github.com/vmware-labs/vmware-customer-connect-cli/releases/download/v1.1.2/vcc-linux-v1.1.2 install vcc ${ LOCALBIN } /vcc Some of the following steps require you to download software from Customer Connect. It is therefore necessary to set authentication credentials for vcc as environment variables VCC_USER and VCC_PASS , i.e.: export VCC_USER = \"my-email-address@my-domain.com\" export VCC_PASS = 'secret-P4ss' Just make sure that you surround values with single-quotes and escape single-quotes characters in them. multi-factor authentication is disabled in your customerconnect profile, otherwise the CLI tool won't work (yet) govc \u00b6 govc allows you to interact with the vSphere vCenter appliance via CLI, thus being able to script your way to the configuration of your environment. Select the proper version/platform from https://github.com/vmware/govmomi/releases and install it, i.e.: curl -sSfL -o govc.tar.gz https://github.com/vmware/govmomi/releases/download/v0.30.2/govc_Linux_x86_64.tar.gz tar xzvf govc.tar.gz govc install govc ${ LOCALBIN } /govc Carvel suite \u00b6 The Carvel suite is a set of useful tools for dealing with YAML files and OCI images. curl -L https://carvel.dev/install.sh | K14SIO_INSTALL_BIN_DIR = ${ LOCALBIN } bash tanzu CLI \u00b6 The tanzu CLI is available within the TKG product. Using vcc you can browse the products/subproducts/versions/files hierarchy and download exactly what you need. For example, # list all the available products vcc get products # list all the available subproducts belonging to the vmware_tanzu_kubernetes_grid product vcc get subproducts -p vmware_tanzu_kubernetes_grid # list all the versions for the subproduct tkg vcc get versions -p vmware_tanzu_kubernetes_grid -s tkg # list all the files available for version 2.1.0 # at this point vcc does the actual login vcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 # download the tanzu cli vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 -f 'tanzu-cli-bundle-linux-amd64.tar.gz' --accepteula The downloaded files are available in the ~/vcc-downloads directory. Now you can install the tanzu CLI: tar xvzf vcc-downloads/tanzu-cli-bundle-linux-amd64.tar.gz install cli/core/v0.28.0/tanzu-core-linux_amd64 ${ LOCALBIN } /tanzu tanzu init","title":"Prepare bastion host"},{"location":"tkgm/bastion/#prepare-bastion-host","text":"You will need a Linux bastion host equipped with a few tools to be able to fully deploy and operate TKGM. You can pick any distribution of choice, as long as you know how to do a basic configuration and deploy software on it. However, in this guide, I provide guidance on Ubuntu and RHEL-like (i.e. RHEL, CentOS, Oracle Linux) distributions only. It is also recommended not to use the root user, but instead create an unprivileged user (belonging to wheel group) and use it for day1 and day2 operations. In this guide I am using the tanzu user, that can be created as useradd -m -G wheel tanzu To allow the bastion host to relocate TKGM images , the minimum requirements for the bastion host are 2GB RAM, 2 vCPU and 45 GB free hard disk space, therefore it's better to provide at least ~10GB more for OS and packages installation. If you have fine-grained file systems, make sure you grant at least 20 GB free disk space for the /tmp directory, for storing temporary files during images relocation.","title":"Prepare bastion host"},{"location":"tkgm/bastion/#proxy-configuration-optional","text":"NSX-T can be used to create segregated networks with proper policies to block egress traffic, and simulate an air-gapped environment for the TKGM cluster The bastion host is used to fetch the required images and copy them to the TKGM isolated network, therefore it somehow needs to access the Internet, i.e. via a proxy. For simulating a proxy we can use another host running a basic Squid installation. On Ubuntu, it can be installed as apt install -y squid By default, Squid denies all outgoing connections if not explicitly authorised via a http_access directive, like acl external_allowed dstdomain \"/etc/squid/conf.d/allowed-domains.txt\" http_access allow external_allowed","title":"Proxy configuration (optional)"},{"location":"tkgm/bastion/#software-installation","text":"First of all, you need to identify a directory where you want to store those binaries. In the following guide, we'll be using the LOCALBIN variable you can set to whatever directory you are comfortable with, but make sure you also add it to your user's PATH environment variable. Also, if writing to it requires superuser privileges, do not forget to prepend every write command with sudo . LOCALBIN = \" $HOME /.local/bin\" mkdir -p $LOCALBIN export PATH = $PATH : $LOCALBIN","title":"Software installation"},{"location":"tkgm/bastion/#os-tools","text":"Oracle Linux sudo dnf install -y bash-completion bind-utils net-tools traceroute","title":"OS tools"},{"location":"tkgm/bastion/#git","text":"Most distributions come with git pre-installed, but in case yours doesn't you can leverage your package manager to get it: Oracle Linux Ubuntu sudo dnf install -y git-all sudo apt install -y git","title":"git"},{"location":"tkgm/bastion/#kubectl","text":"The latest kubectl release can be installed as curl -sSfLO \"https://dl.k8s.io/release/ $( curl -sSfL https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" install kubectl ${ LOCALBIN } /kubectl","title":"kubectl"},{"location":"tkgm/bastion/#kubectx-and-kubens","text":"Helper scripts to easily switch kubeconfig context and set default namespace. You may want to get the scripts from an actual release and not from the main branch. LATEST = \"v0.9.4\" sudo git clone --depth 1 --branch $LATEST -c advice.detachedHead = false https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -sf /opt/kubectx/kubectx ${ LOCALBIN } /kubectx sudo ln -sf /opt/kubectx/kubens ${ LOCALBIN } /kubens","title":"kubectx and kubens"},{"location":"tkgm/bastion/#helm","text":"curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | HELM_INSTALL_DIR = ~/.local/bin bash","title":"helm"},{"location":"tkgm/bastion/#fzf","text":"fzf is a general-purpose command-line fuzzy finder. curl -sSfL https://github.com/junegunn/fzf/releases/download/0.38.0/fzf-0.38.0-linux_amd64.tar.gz | tar xzf - install fzf ${ LOCALBIN } /fzf","title":"fzf"},{"location":"tkgm/bastion/#yq","text":"curl -sSfL -o yq https://github.com/mikefarah/yq/releases/download/v4.30.8/yq_linux_amd64 install yq ${ LOCALBIN } /yq","title":"yq"},{"location":"tkgm/bastion/#jq","text":"Oracle Linux Ubuntu sudo dnf install -y jq sudo apt install -y jq","title":"jq"},{"location":"tkgm/bastion/#docker","text":"Oracle Linux sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo dnf update -y sudo dnf install -y docker-ce docker-ce-cli containerd.io Start and enable the docker service: sudo systemctl start docker sudo systemctl enable docker Finally, add the user to the docker group: sudo gpasswd -a tanzu docker","title":"docker"},{"location":"tkgm/bastion/#vcc","text":"vcc is a useful CLI tool for downloading packages from customerconnect.vmware.com. Get the latest release from https://github.com/vmware-labs/vmware-customer-connect-cli/releases , i.e.: curl -sSfL -o vcc https://github.com/vmware-labs/vmware-customer-connect-cli/releases/download/v1.1.2/vcc-linux-v1.1.2 install vcc ${ LOCALBIN } /vcc Some of the following steps require you to download software from Customer Connect. It is therefore necessary to set authentication credentials for vcc as environment variables VCC_USER and VCC_PASS , i.e.: export VCC_USER = \"my-email-address@my-domain.com\" export VCC_PASS = 'secret-P4ss' Just make sure that you surround values with single-quotes and escape single-quotes characters in them. multi-factor authentication is disabled in your customerconnect profile, otherwise the CLI tool won't work (yet)","title":"vcc"},{"location":"tkgm/bastion/#govc","text":"govc allows you to interact with the vSphere vCenter appliance via CLI, thus being able to script your way to the configuration of your environment. Select the proper version/platform from https://github.com/vmware/govmomi/releases and install it, i.e.: curl -sSfL -o govc.tar.gz https://github.com/vmware/govmomi/releases/download/v0.30.2/govc_Linux_x86_64.tar.gz tar xzvf govc.tar.gz govc install govc ${ LOCALBIN } /govc","title":"govc"},{"location":"tkgm/bastion/#carvel-suite","text":"The Carvel suite is a set of useful tools for dealing with YAML files and OCI images. curl -L https://carvel.dev/install.sh | K14SIO_INSTALL_BIN_DIR = ${ LOCALBIN } bash","title":"Carvel suite"},{"location":"tkgm/bastion/#tanzu-cli","text":"The tanzu CLI is available within the TKG product. Using vcc you can browse the products/subproducts/versions/files hierarchy and download exactly what you need. For example, # list all the available products vcc get products # list all the available subproducts belonging to the vmware_tanzu_kubernetes_grid product vcc get subproducts -p vmware_tanzu_kubernetes_grid # list all the versions for the subproduct tkg vcc get versions -p vmware_tanzu_kubernetes_grid -s tkg # list all the files available for version 2.1.0 # at this point vcc does the actual login vcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 # download the tanzu cli vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 -f 'tanzu-cli-bundle-linux-amd64.tar.gz' --accepteula The downloaded files are available in the ~/vcc-downloads directory. Now you can install the tanzu CLI: tar xvzf vcc-downloads/tanzu-cli-bundle-linux-amd64.tar.gz install cli/core/v0.28.0/tanzu-core-linux_amd64 ${ LOCALBIN } /tanzu tanzu init","title":"tanzu CLI"},{"location":"tkgm/identity-management/","text":"Identity management \u00b6 Info The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-iam-index.html . This guide focuses on OIDC authentication and, as such, leverages Okta developer platform as an OIDC-compliant IdP . For the moment, it takes a ClickOps approach for testing things quickly, but in the future it is planned to switch to a more scripted and IaC-oriented methodology. An Okta Terraform provider is available, indeed, so the plan is to eventually configure everything with it, following this blog post . Pinniped is the component used for authenticating against LDAP or OIDC endpoints, and comes as a package in TKG. It is silently installed at deploy time, but can be configured either during management cluster deployment or afterwards. Configure Okta IdP \u00b6 You can sign up to an Okta developer account to get started quickly with an OIDC-compliant identity provider, and run some authentication and authorisation tests from your TKG platform. The simplest use case is to have users authenticate against Okta, retrieve the groups they belong to and assign Kubernetes permissions based on groups. RBAC is still configured on the Kubernetes end, whilst the authentication part is completely delegated to the IdP . Create a new application \u00b6 Log into your Okta developer account and go to Applications -> Applications -> Create a new app integration , select OIDC - OpenID Connect as sign-in method and Web Application as application type. Pick a name for the app (i.e. tkgm ), enable the refresh token amd save; you will have to set the sign-in redirect URI when pinniped is up and running. Save the Client ID and the Client Secret, you will need them for configuring pinniped. Create users and groups \u00b6 Now you may want to configure a few groups in Directory -> Groups and assign people to them, as well as the application you just created, so that group members will be able to authenticate to your cluster. Initially you have just one user, whose username is the email address you registered to Okta with, but you can also create more if you wish, if you want to simulate a multi-user multi-group scenario. Configure authorization server \u00b6 You need to ensure that group memberships are passed along in the JWT token when a user authenticates, and you must instruct the authorization server to do so. Go to Security -> API and hit the pencil next to the default authorization server to modify it. Go to Claims and add a new claim: the name must be groups , included in ID Token type Always , and you may want to pass all the groups, with no filtering whatsoever. So the value type must be Groups and the filter Matches regex .* , and it must be included in Any scope . You can test the settings in the Token Preview tab. Further details are available in the official Okta docs . Configure pinniped during management cluster deployment \u00b6 There are a few variables you need to set during management cluster installation, either via the GUI or the flat configuration file, as described in the section about TKG management cluster . Such variables, as described also in the VMware official docs , are IDENTITY_MANAGEMENT_TYPE : oidc OIDC_IDENTITY_PROVIDER_CLIENT_ID : <OIDC-CLIENT-ID> OIDC_IDENTITY_PROVIDER_CLIENT_SECRET : <OIDC-CLIENT-SECRET> OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM : groups OIDC_IDENTITY_PROVIDER_ISSUER_URL : https://<OIDC_ENDPOINT> OIDC_IDENTITY_PROVIDER_NAME : <IDP-NAME> OIDC_IDENTITY_PROVIDER_SCOPES : email,profile,groups OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM : email They must be set in the flat configuration file and the deployment process will take care of the rest, generating TLS certificates too. For publicly trusted certificates (or at least within the enterprise, if a private CA is available) there will be a dedicated section. Note Those variables need to be set only in the management cluster; in fact, all workload clusters will inherit the same configuration from their own management cluster. Configure pinniped after management cluster deployment \u00b6 OIDC authentication can be activated on existing management clusters, too. In fact, whether or not OIDC configs are passed to the management cluster during deployment, a PackageInstall resource for pinniped is created, but its reconciliation fails until a specific secret, containing the package's configuration values, is created in the tkg-system namespace. kapp-controller is on the lookout for such a secret and completes the package installation as soon as it is available. To get the secret manifest generated, you need to mock a flat configuration file and fake a TKG cluster creation, in order to get back the secret's YAML definition. Make a copy of your previous management cluster flat file and add the variables defined in the previous paragraph . Set a fake value for the VSPHERE_CONTROL_PLANE_ENDPOINT variable, to an unused IP, like VSPHERE_CONTROL_PLANE_ENDPOINT : 1.1.1.1 otherwise the Tanzu CLI would fail during pre-flight checks, because the original endpoint is already taken by the existing cluster. Then set a few environment variables: export IDENTITY_MANAGEMENT_TYPE = oidc export _TKG_CLUSTER_FORCE_ROLE = \"management\" export FILTER_BY_ADDON_TYPE = \"authentication/pinniped\" Then generate the secret's YAML manifest: tanzu cluster create <CLUSTER-NAME> --dry-run --file /path/to/the/yaml/config/file > pinniped-secret.yaml Make sure you pick the actual cluster name, as shown in kubectl get clusters.cluster.x-k8s.io -n tkg-system Do to take a look at the generated pinniped-secret.yaml file and change it if your cluster is behind a proxy. In fact, proxy configurations do not get passed along to the pinniped values, and so you have to set the values for: http_proxy https_proxy no_proxy The no_proxy variable is a comma-separated list of IP or domains patterns ( this is worth a read). Remember to add to it also the CIDR range of your services network (i.e. 100.64.0.0/13 ). Then you must apply the YAML manifest to the cluster kubectl apply -f pinniped-secret.yaml You can monitor the status of the configuration looking at the resources in the pinniped-supervisor namespace. In the end, you should have something like \u276f kubectl -n pinniped-supervisor get all NAME READY STATUS RESTARTS AGE pod/pinniped-post-deploy-controller-64c59657c5-hsd5w 1 /1 Running 0 109m pod/pinniped-post-deploy-job-scstf 0 /1 Completed 0 59m pod/pinniped-supervisor-77d67dc647-f7bpd 1 /1 Running 0 59m pod/pinniped-supervisor-77d67dc647-t8fqq 1 /1 Running 0 59m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/pinniped-supervisor NodePort 100 .69.192.203 <none> 443 :31234/TCP 109m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/pinniped-post-deploy-controller 1 /1 1 1 109m deployment.apps/pinniped-supervisor 2 /2 2 2 109m NAME DESIRED CURRENT READY AGE replicaset.apps/pinniped-post-deploy-controller-64c59657c5 1 1 1 109m replicaset.apps/pinniped-supervisor-77d67dc647 2 2 2 109m NAME COMPLETIONS DURATION AGE job.batch/pinniped-post-deploy-job 1 /1 8s 59m It is worth to check the pinniped-concierge namespace as well: \u276f kubectl -n pinniped-concierge get all NAME READY STATUS RESTARTS AGE pod/pinniped-concierge-5f85876ff6-fpq9t 1 /1 Running 0 111m pod/pinniped-concierge-5f85876ff6-n497d 1 /1 Running 0 111m pod/pinniped-concierge-kube-cert-agent-f6fbff54f-7jtfp 1 /1 Running 0 110m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/pinniped-concierge-api ClusterIP 100 .71.104.69 <none> 443 /TCP 111m service/pinniped-concierge-proxy ClusterIP 100 .71.213.71 <none> 443 /TCP 111m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/pinniped-concierge 2 /2 2 2 111m deployment.apps/pinniped-concierge-kube-cert-agent 1 /1 1 1 110m NAME DESIRED CURRENT READY AGE replicaset.apps/pinniped-concierge-5f85876ff6 2 2 2 111m replicaset.apps/pinniped-concierge-kube-cert-agent-f6fbff54f 1 1 1 110m Info Using Kube-VIP, the only way to expose pinniped is via a NodePort service, thus do remember to set the appropriate rules in your network firewalls to allow the traffic to port 31234/tcp. Provide own TLS certificates \u00b6 https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-iam-custom-pinniped-certificates.html Configure Okta redirect URI \u00b6 Now that you have pinniped configured and running, you can go back to your Okta account and set the redirect URI for your application. Edit the General Settings section of your application and set the sign-in redirect URI to https://<YOUR-MGMT-CLUSTER-ENDPOINT>:31234/callback . Use the very same endpoint you set in your certificate, if you happened to change it. Get the kubeconfig file \u00b6 Use the Tanzu CLI to retrieve the kubeconfig file to authenticate via OIDC: tanzu mc kubeconfig get --export-file /tmp/tkgm.oidc.kubeconfig Open the file and look at the users object; the tanzu pinniped-auth login command is used to authenticate the user, with a few configuration options: make sure the --issuer flag points to the same endpoint as the Okta redirect URI (without the trailing /callback ); add the item --skip-browser to the end of the args list, to prevent the Tanzu CLI from trying to run a browser to let the user authenticate (on SSH-based jumphosts you do not have nor you do want to have a GUI). Run a kubectl get command with the new kubeconfig file to get prompted for authentication \u276f kubectl get pods --kubeconfig ~/t1xl/t1xl.oidc.kubeconfig Log in by visiting this link: https://my.cluster.endpoint:31234/oauth2/authorize?access_type = offline & client_id = pinniped-cli & code_challenge = lGjmpPEfyN0544UEOr2QYWawsWBzKOorlQG-Ws73FHs & code_challenge_method = S256 & nonce = 48cd4fc34b4e789ee8e6e590ae54aab0 & redirect_uri = http%3A%2F%2F127.0.0.1%3A45359%2Fcallback & response_mode = form_post & response_type = code & scope = offline_access+openid+pinniped%3Arequest-audience & state = 93b02cc91ad38b8e905844abcf0c6144 Optionally, paste your authorization code: Paste the URL in your browser, authenticate to Okta and you get a code back that you have to paste into the console: you are now authenticated. You can see your token details (including the groups that have been passed along) at ~/.config/tanzu/pinniped/sessions.yaml . Your request is going to fail unless you have already configured RBAC on your cluster. Configure RBAC \u00b6 If you have correctly configured users and groups and the authorization server , you should now find a list of groups in the ~/.config/tanzu/pinniped/sessions.yaml file for your TKG ID token. The authorization part still lies on the Kubernetes part, leveraging Role s and ClusterRole s for defining permissions, and RoleBinding s and ClusterRoleBinding s for granting them to either users or groups. TKG comes with a few pre-defined ClusterRole s cluster-admin: Full access to the cluster. When used in a RoleBinding, this role gives full access to any resource in the namespace specified in the binding. admin: Admin access to most resources in a namespace. Can create and modify roles and role bindings within the namespace. edit: Read-write access to most objects in a namespace, such as deployments, services, and pods. Cannot view, create, or modify roles and role bindings. view: Read-only access to most objects in a namespace. Cannot view, create, or modify roles and role bindings More info at VMware docs . As an example, a simple ClusterRoleBinding that grants a group view permissions on all namespaces can be defined as: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : dev-view roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : dev","title":"Identity management"},{"location":"tkgm/identity-management/#identity-management","text":"Info The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-iam-index.html . This guide focuses on OIDC authentication and, as such, leverages Okta developer platform as an OIDC-compliant IdP . For the moment, it takes a ClickOps approach for testing things quickly, but in the future it is planned to switch to a more scripted and IaC-oriented methodology. An Okta Terraform provider is available, indeed, so the plan is to eventually configure everything with it, following this blog post . Pinniped is the component used for authenticating against LDAP or OIDC endpoints, and comes as a package in TKG. It is silently installed at deploy time, but can be configured either during management cluster deployment or afterwards.","title":"Identity management"},{"location":"tkgm/identity-management/#configure-okta-idp","text":"You can sign up to an Okta developer account to get started quickly with an OIDC-compliant identity provider, and run some authentication and authorisation tests from your TKG platform. The simplest use case is to have users authenticate against Okta, retrieve the groups they belong to and assign Kubernetes permissions based on groups. RBAC is still configured on the Kubernetes end, whilst the authentication part is completely delegated to the IdP .","title":"Configure Okta IdP"},{"location":"tkgm/identity-management/#create-a-new-application","text":"Log into your Okta developer account and go to Applications -> Applications -> Create a new app integration , select OIDC - OpenID Connect as sign-in method and Web Application as application type. Pick a name for the app (i.e. tkgm ), enable the refresh token amd save; you will have to set the sign-in redirect URI when pinniped is up and running. Save the Client ID and the Client Secret, you will need them for configuring pinniped.","title":"Create a new application"},{"location":"tkgm/identity-management/#create-users-and-groups","text":"Now you may want to configure a few groups in Directory -> Groups and assign people to them, as well as the application you just created, so that group members will be able to authenticate to your cluster. Initially you have just one user, whose username is the email address you registered to Okta with, but you can also create more if you wish, if you want to simulate a multi-user multi-group scenario.","title":"Create users and groups"},{"location":"tkgm/identity-management/#configure-authorization-server","text":"You need to ensure that group memberships are passed along in the JWT token when a user authenticates, and you must instruct the authorization server to do so. Go to Security -> API and hit the pencil next to the default authorization server to modify it. Go to Claims and add a new claim: the name must be groups , included in ID Token type Always , and you may want to pass all the groups, with no filtering whatsoever. So the value type must be Groups and the filter Matches regex .* , and it must be included in Any scope . You can test the settings in the Token Preview tab. Further details are available in the official Okta docs .","title":"Configure authorization server"},{"location":"tkgm/identity-management/#configure-pinniped-during-management-cluster-deployment","text":"There are a few variables you need to set during management cluster installation, either via the GUI or the flat configuration file, as described in the section about TKG management cluster . Such variables, as described also in the VMware official docs , are IDENTITY_MANAGEMENT_TYPE : oidc OIDC_IDENTITY_PROVIDER_CLIENT_ID : <OIDC-CLIENT-ID> OIDC_IDENTITY_PROVIDER_CLIENT_SECRET : <OIDC-CLIENT-SECRET> OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM : groups OIDC_IDENTITY_PROVIDER_ISSUER_URL : https://<OIDC_ENDPOINT> OIDC_IDENTITY_PROVIDER_NAME : <IDP-NAME> OIDC_IDENTITY_PROVIDER_SCOPES : email,profile,groups OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM : email They must be set in the flat configuration file and the deployment process will take care of the rest, generating TLS certificates too. For publicly trusted certificates (or at least within the enterprise, if a private CA is available) there will be a dedicated section. Note Those variables need to be set only in the management cluster; in fact, all workload clusters will inherit the same configuration from their own management cluster.","title":"Configure pinniped during management cluster deployment"},{"location":"tkgm/identity-management/#configure-pinniped-after-management-cluster-deployment","text":"OIDC authentication can be activated on existing management clusters, too. In fact, whether or not OIDC configs are passed to the management cluster during deployment, a PackageInstall resource for pinniped is created, but its reconciliation fails until a specific secret, containing the package's configuration values, is created in the tkg-system namespace. kapp-controller is on the lookout for such a secret and completes the package installation as soon as it is available. To get the secret manifest generated, you need to mock a flat configuration file and fake a TKG cluster creation, in order to get back the secret's YAML definition. Make a copy of your previous management cluster flat file and add the variables defined in the previous paragraph . Set a fake value for the VSPHERE_CONTROL_PLANE_ENDPOINT variable, to an unused IP, like VSPHERE_CONTROL_PLANE_ENDPOINT : 1.1.1.1 otherwise the Tanzu CLI would fail during pre-flight checks, because the original endpoint is already taken by the existing cluster. Then set a few environment variables: export IDENTITY_MANAGEMENT_TYPE = oidc export _TKG_CLUSTER_FORCE_ROLE = \"management\" export FILTER_BY_ADDON_TYPE = \"authentication/pinniped\" Then generate the secret's YAML manifest: tanzu cluster create <CLUSTER-NAME> --dry-run --file /path/to/the/yaml/config/file > pinniped-secret.yaml Make sure you pick the actual cluster name, as shown in kubectl get clusters.cluster.x-k8s.io -n tkg-system Do to take a look at the generated pinniped-secret.yaml file and change it if your cluster is behind a proxy. In fact, proxy configurations do not get passed along to the pinniped values, and so you have to set the values for: http_proxy https_proxy no_proxy The no_proxy variable is a comma-separated list of IP or domains patterns ( this is worth a read). Remember to add to it also the CIDR range of your services network (i.e. 100.64.0.0/13 ). Then you must apply the YAML manifest to the cluster kubectl apply -f pinniped-secret.yaml You can monitor the status of the configuration looking at the resources in the pinniped-supervisor namespace. In the end, you should have something like \u276f kubectl -n pinniped-supervisor get all NAME READY STATUS RESTARTS AGE pod/pinniped-post-deploy-controller-64c59657c5-hsd5w 1 /1 Running 0 109m pod/pinniped-post-deploy-job-scstf 0 /1 Completed 0 59m pod/pinniped-supervisor-77d67dc647-f7bpd 1 /1 Running 0 59m pod/pinniped-supervisor-77d67dc647-t8fqq 1 /1 Running 0 59m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/pinniped-supervisor NodePort 100 .69.192.203 <none> 443 :31234/TCP 109m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/pinniped-post-deploy-controller 1 /1 1 1 109m deployment.apps/pinniped-supervisor 2 /2 2 2 109m NAME DESIRED CURRENT READY AGE replicaset.apps/pinniped-post-deploy-controller-64c59657c5 1 1 1 109m replicaset.apps/pinniped-supervisor-77d67dc647 2 2 2 109m NAME COMPLETIONS DURATION AGE job.batch/pinniped-post-deploy-job 1 /1 8s 59m It is worth to check the pinniped-concierge namespace as well: \u276f kubectl -n pinniped-concierge get all NAME READY STATUS RESTARTS AGE pod/pinniped-concierge-5f85876ff6-fpq9t 1 /1 Running 0 111m pod/pinniped-concierge-5f85876ff6-n497d 1 /1 Running 0 111m pod/pinniped-concierge-kube-cert-agent-f6fbff54f-7jtfp 1 /1 Running 0 110m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/pinniped-concierge-api ClusterIP 100 .71.104.69 <none> 443 /TCP 111m service/pinniped-concierge-proxy ClusterIP 100 .71.213.71 <none> 443 /TCP 111m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/pinniped-concierge 2 /2 2 2 111m deployment.apps/pinniped-concierge-kube-cert-agent 1 /1 1 1 110m NAME DESIRED CURRENT READY AGE replicaset.apps/pinniped-concierge-5f85876ff6 2 2 2 111m replicaset.apps/pinniped-concierge-kube-cert-agent-f6fbff54f 1 1 1 110m Info Using Kube-VIP, the only way to expose pinniped is via a NodePort service, thus do remember to set the appropriate rules in your network firewalls to allow the traffic to port 31234/tcp.","title":"Configure pinniped after management cluster deployment"},{"location":"tkgm/identity-management/#provide-own-tls-certificates","text":"https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-iam-custom-pinniped-certificates.html","title":"Provide own TLS certificates"},{"location":"tkgm/identity-management/#configure-okta-redirect-uri","text":"Now that you have pinniped configured and running, you can go back to your Okta account and set the redirect URI for your application. Edit the General Settings section of your application and set the sign-in redirect URI to https://<YOUR-MGMT-CLUSTER-ENDPOINT>:31234/callback . Use the very same endpoint you set in your certificate, if you happened to change it.","title":"Configure Okta redirect URI"},{"location":"tkgm/identity-management/#get-the-kubeconfig-file","text":"Use the Tanzu CLI to retrieve the kubeconfig file to authenticate via OIDC: tanzu mc kubeconfig get --export-file /tmp/tkgm.oidc.kubeconfig Open the file and look at the users object; the tanzu pinniped-auth login command is used to authenticate the user, with a few configuration options: make sure the --issuer flag points to the same endpoint as the Okta redirect URI (without the trailing /callback ); add the item --skip-browser to the end of the args list, to prevent the Tanzu CLI from trying to run a browser to let the user authenticate (on SSH-based jumphosts you do not have nor you do want to have a GUI). Run a kubectl get command with the new kubeconfig file to get prompted for authentication \u276f kubectl get pods --kubeconfig ~/t1xl/t1xl.oidc.kubeconfig Log in by visiting this link: https://my.cluster.endpoint:31234/oauth2/authorize?access_type = offline & client_id = pinniped-cli & code_challenge = lGjmpPEfyN0544UEOr2QYWawsWBzKOorlQG-Ws73FHs & code_challenge_method = S256 & nonce = 48cd4fc34b4e789ee8e6e590ae54aab0 & redirect_uri = http%3A%2F%2F127.0.0.1%3A45359%2Fcallback & response_mode = form_post & response_type = code & scope = offline_access+openid+pinniped%3Arequest-audience & state = 93b02cc91ad38b8e905844abcf0c6144 Optionally, paste your authorization code: Paste the URL in your browser, authenticate to Okta and you get a code back that you have to paste into the console: you are now authenticated. You can see your token details (including the groups that have been passed along) at ~/.config/tanzu/pinniped/sessions.yaml . Your request is going to fail unless you have already configured RBAC on your cluster.","title":"Get the kubeconfig file"},{"location":"tkgm/identity-management/#configure-rbac","text":"If you have correctly configured users and groups and the authorization server , you should now find a list of groups in the ~/.config/tanzu/pinniped/sessions.yaml file for your TKG ID token. The authorization part still lies on the Kubernetes part, leveraging Role s and ClusterRole s for defining permissions, and RoleBinding s and ClusterRoleBinding s for granting them to either users or groups. TKG comes with a few pre-defined ClusterRole s cluster-admin: Full access to the cluster. When used in a RoleBinding, this role gives full access to any resource in the namespace specified in the binding. admin: Admin access to most resources in a namespace. Can create and modify roles and role bindings within the namespace. edit: Read-write access to most objects in a namespace, such as deployments, services, and pods. Cannot view, create, or modify roles and role bindings. view: Read-only access to most objects in a namespace. Cannot view, create, or modify roles and role bindings More info at VMware docs . As an example, a simple ClusterRoleBinding that grants a group view permissions on all namespaces can be defined as: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : dev-view roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : dev","title":"Configure RBAC"},{"location":"tkgm/ingress/","text":"Ingress controller \u00b6 Kube-VIP Load Balancer \u00b6 This topic describes using Kube-VIP as a load balancer for workloads hosted on Tanzu Kubernetes Grid (TKG) workload clusters. Technical Preview This feature is still a technical preview, yet very helpful to quickly create a load balancer to be used in a lab environment, and that will allegedly be fully supported on TKG future releases. To learn how to set it up see official VMware docs . NGINX ingress controller \u00b6 You can install the NGINX ingress controller in a number of ways, detailed in the NGINX docs website . Installation via Helm chart \u00b6 You can pull the NGINX ingress sources from GitHub git clone https://github.com/nginxinc/kubernetes-ingress.git --branch v3.0.2 cd kubernetes-ingress/deployments/helm-chart or install a Helm repo and pull the chart helm repo add nginx-stable https://helm.nginx.com/stable helm repo update helm pull nginx-stable/nginx-ingress --destination . --untar cd nginx-ingress Either way, if your TKG instance lives in an air-gapped environment, you have to download the helm chart from an Internet-connected host and then copy it over to the bastion host inside the environment. You also need to relocate the nginx/nginx-ingress image to the internal registry. Don't forget to provide DockerHub authentication credentials (create a read-only temporary Personal Access Token) to avoid getting throttled . export IMGPKG_REGISTRY_HOSTNAME_0 = '<REGISTRY-FQDN>' export IMGPKG_REGISTRY_USERNAME_0 = '<REGISTRY-USERNAME>' export IMGPKG_REGISTRY_PASSWORD_0 = '<REGISTRY-PASSWORD>' export IMGPKG_REGISTRY_HOSTNAME_1 = 'index.docker.io' export IMGPKG_REGISTRY_USERNAME_1 = '<DOCKERHUB-USERNAME>' export IMGPKG_REGISTRY_PASSWORD_1 = '<DOCKERHUB-PAT>' imgpkg copy -i nginx/nginx-ingress:3.0.2 --to-repo registry.my.domain/apps/nginx-ingress --registry-ca-cert-path harbor-ca.pem Provide a values file for the helm installation to specify your custom settings, i.e.: the URL to the relocated image; the service.externalTrafficPolicy set to Cluster ; the service type and IP address, to make sure it is static and won't change. controller : image : repository : registry.my.domain/apps/nginx-ingress tag : 3.0.2 service : type : LoadBalancer loadBalancerIP : 172.31.9.21 externalTrafficPolicy : Cluster Brief explanation about the externalTrafficPolicy The helm chart's default value is Local , which allows to preserve the client IP address but also keeps the connection local to the node, with no chance for it to be routed to other nodes in the cluster. However, with the default setting, using a load balancer such as Kube-VIP that allocates IP addresses to the control plane nodes (thus assigned to the services of type LoadBalancer ), makes it impossible for external connections to reach the actual application pods, as they do not and never will run on any control plane nodes. See also Kubernetes docs . You can also customise it further if you want; all the available parameters can be listed as helm show values . Finally, you can start the installation: helm install nginx-ingress . --values /path/to/the/values/file.yaml Deploy test HTTP application \u00b6 For testing, you can deploy a NGINX web server and expose its default welcome page through the ingress controller, to check connectivity from both within the cluster and outside of it. First of all, you have to relocate the image ( nginx/nginx:stable-alpine , in this example) to the internal registry like you did for the ingress controller image. Then, you can create a Deployment and its Service deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : labels : app : app1 name : app1 spec : replicas : 2 selector : matchLabels : app : app1 template : metadata : labels : app : app1 spec : containers : - image : registry.my.domain/apps/nginx:stable-alpine name : nginx --- apiVersion : v1 kind : Service metadata : labels : app : app1 name : app1 spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : app1 and an Ingress resource accordingly ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : app1 annotations : nginx.org/rewrites : \"serviceName=app1 rewrite=/\" spec : ingressClassName : nginx rules : - host : \"*.my.domain\" http : paths : - path : /app1 pathType : Prefix backend : service : name : app1 port : number : 80 In this example, the Ingress resource uses a wildcard to listen on all hosts (that end up being associated to the ingress controller IP address), and allows you to define multiple paths that can route the traffic to multiple backend applications. You should now create a DNS record to associate app1.my.domain to the ingress controller service's IP address 172.31.9.21 , or you could just forge the Host header in the test HTTP request: \u276f curl -H \"Host: app1.my.domain\" http://172.31.9.21/app1 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark ; } body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href = \"http://nginx.org/\" >nginx.org</a>.<br/> Commercial support is available at <a href = \"http://nginx.com/\" >nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> This proves that the configuration is correct.","title":"Ingress controller"},{"location":"tkgm/ingress/#ingress-controller","text":"","title":"Ingress controller"},{"location":"tkgm/ingress/#kube-vip-load-balancer","text":"This topic describes using Kube-VIP as a load balancer for workloads hosted on Tanzu Kubernetes Grid (TKG) workload clusters. Technical Preview This feature is still a technical preview, yet very helpful to quickly create a load balancer to be used in a lab environment, and that will allegedly be fully supported on TKG future releases. To learn how to set it up see official VMware docs .","title":"Kube-VIP Load Balancer"},{"location":"tkgm/ingress/#nginx-ingress-controller","text":"You can install the NGINX ingress controller in a number of ways, detailed in the NGINX docs website .","title":"NGINX ingress controller"},{"location":"tkgm/ingress/#installation-via-helm-chart","text":"You can pull the NGINX ingress sources from GitHub git clone https://github.com/nginxinc/kubernetes-ingress.git --branch v3.0.2 cd kubernetes-ingress/deployments/helm-chart or install a Helm repo and pull the chart helm repo add nginx-stable https://helm.nginx.com/stable helm repo update helm pull nginx-stable/nginx-ingress --destination . --untar cd nginx-ingress Either way, if your TKG instance lives in an air-gapped environment, you have to download the helm chart from an Internet-connected host and then copy it over to the bastion host inside the environment. You also need to relocate the nginx/nginx-ingress image to the internal registry. Don't forget to provide DockerHub authentication credentials (create a read-only temporary Personal Access Token) to avoid getting throttled . export IMGPKG_REGISTRY_HOSTNAME_0 = '<REGISTRY-FQDN>' export IMGPKG_REGISTRY_USERNAME_0 = '<REGISTRY-USERNAME>' export IMGPKG_REGISTRY_PASSWORD_0 = '<REGISTRY-PASSWORD>' export IMGPKG_REGISTRY_HOSTNAME_1 = 'index.docker.io' export IMGPKG_REGISTRY_USERNAME_1 = '<DOCKERHUB-USERNAME>' export IMGPKG_REGISTRY_PASSWORD_1 = '<DOCKERHUB-PAT>' imgpkg copy -i nginx/nginx-ingress:3.0.2 --to-repo registry.my.domain/apps/nginx-ingress --registry-ca-cert-path harbor-ca.pem Provide a values file for the helm installation to specify your custom settings, i.e.: the URL to the relocated image; the service.externalTrafficPolicy set to Cluster ; the service type and IP address, to make sure it is static and won't change. controller : image : repository : registry.my.domain/apps/nginx-ingress tag : 3.0.2 service : type : LoadBalancer loadBalancerIP : 172.31.9.21 externalTrafficPolicy : Cluster Brief explanation about the externalTrafficPolicy The helm chart's default value is Local , which allows to preserve the client IP address but also keeps the connection local to the node, with no chance for it to be routed to other nodes in the cluster. However, with the default setting, using a load balancer such as Kube-VIP that allocates IP addresses to the control plane nodes (thus assigned to the services of type LoadBalancer ), makes it impossible for external connections to reach the actual application pods, as they do not and never will run on any control plane nodes. See also Kubernetes docs . You can also customise it further if you want; all the available parameters can be listed as helm show values . Finally, you can start the installation: helm install nginx-ingress . --values /path/to/the/values/file.yaml","title":"Installation via Helm chart"},{"location":"tkgm/ingress/#deploy-test-http-application","text":"For testing, you can deploy a NGINX web server and expose its default welcome page through the ingress controller, to check connectivity from both within the cluster and outside of it. First of all, you have to relocate the image ( nginx/nginx:stable-alpine , in this example) to the internal registry like you did for the ingress controller image. Then, you can create a Deployment and its Service deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : labels : app : app1 name : app1 spec : replicas : 2 selector : matchLabels : app : app1 template : metadata : labels : app : app1 spec : containers : - image : registry.my.domain/apps/nginx:stable-alpine name : nginx --- apiVersion : v1 kind : Service metadata : labels : app : app1 name : app1 spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : app1 and an Ingress resource accordingly ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : app1 annotations : nginx.org/rewrites : \"serviceName=app1 rewrite=/\" spec : ingressClassName : nginx rules : - host : \"*.my.domain\" http : paths : - path : /app1 pathType : Prefix backend : service : name : app1 port : number : 80 In this example, the Ingress resource uses a wildcard to listen on all hosts (that end up being associated to the ingress controller IP address), and allows you to define multiple paths that can route the traffic to multiple backend applications. You should now create a DNS record to associate app1.my.domain to the ingress controller service's IP address 172.31.9.21 , or you could just forge the Host header in the test HTTP request: \u276f curl -H \"Host: app1.my.domain\" http://172.31.9.21/app1 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark ; } body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href = \"http://nginx.org/\" >nginx.org</a>.<br/> Commercial support is available at <a href = \"http://nginx.com/\" >nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> This proves that the configuration is correct.","title":"Deploy test HTTP application"},{"location":"tkgm/management-cluster/","text":"TKG management cluster \u00b6 Info The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-index.html . Import node OS images \u00b6 TKGm can be run either on Ubuntu or Photon VMs, whose base OS images are available on Customer Connect. \u276f vcc get versions -p vmware_tanzu_kubernetes_grid -s tkg ... FILENAME SIZE BUILD NUMBER DESCRIPTION ... photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova 1 .02 GB 21158328 Photon v3 Kubernetes v1.24.9 OVA photon-3-kube-v1.23.15+vmware.1-tkg.1-2d8c52b5e3dc2b7a03bf3a04815b9474.ova 1022 .45 MB 21158328 Photon v3 Kubernetes v1.23.15 OVA photon-3-kube-v1.22.17+vmware.1-tkg.1-a10ef8e088cc2c89418bca79a2fcc594.ova 1 GB 21158328 Photon v3 Kubernetes v1.22.17 OVA ubuntu-2004-kube-v1.24.9+vmware.1-tkg.1-b030088fe71fea7ff1ecb87a4d425c93.ova 1 .78 GB 21158328 Ubuntu 2004 Kubernetes v1.24.9 OVA ubuntu-2004-kube-v1.23.15+vmware.1-tkg.1-66940e9f3a5eda8bbd51042b1aaea6e1.ova 1 .74 GB 21158328 Ubuntu 2004 Kubernetes v1.23.15 OVA ubuntu-2004-kube-v1.22.17+vmware.1-tkg.1-df08b304658a6cf17f5e74dc0ab7543c.ova 1 .77 GB 21158328 Ubuntu 2004 Kubernetes v1.22.17 OVA ... You might want to select the latest version of a particular flavour, to benefit from the latest updates and getting closer to the most recent upstream version. In this case, we're going to pick the Photon image for Kubernetes v1.24.9 vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 -f 'photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova' --accepteula and import it to our vSphere content library, to make sure it's available when needed. govc library.import -n photon-1.24.9 ova $HOME /vcc-downloads/photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova However, TKGm expects a VM template to be available, therefore we need to deploy a template from this content library item. We dump the specs into a JSON file govc import.spec $HOME /vcc-downloads/photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova > photon.json and we edit it to make sure that it is marked as a template it does not get powered on the DiskProvisioning configuration is correct The result looks like this { \"DiskProvisioning\" : \"thin\" , \"IPAllocationPolicy\" : \"dhcpPolicy\" , \"IPProtocol\" : \"IPv4\" , \"NetworkMapping\" : [ { \"Name\" : \"nic0\" , \"Network\" : \"\" } ], \"Annotation\" : \"Cluster API vSphere image - VMware Photon OS 64-bit and Kubernetes v1.24.9+vmware.1 - https://github.com/kubernetes-sigs/cluster-api-provider-vsphere\" , \"MarkAsTemplate\" : true , \"PowerOn\" : false , \"InjectOvfEnv\" : false , \"WaitForIP\" : false , \"Name\" : null } Finally, we can deploy the OVA into a folder dedicated to our cluster govc folder.create /vc01/vm/tkgm govc library.deploy -options photon.json -folder tkgm /ova/photon-1.24.9 Prepare management cluster configuration \u00b6 Tanzu Kubernetes Grid standalone cluster deployment can be run in unattended mode via the Tanzu CLI, feeding it with a YAML configuration file . Alternatively, a web UI is also available for guiding the user through all the required settings. There is actually a third option, which is preferrable if you are not familiar with or not entirely sure about all the different settings available: it consists of using the web UI for generating the initial YAML file, and then fine tuning it, if needed, before submitting it to the Tanzu CLI. At this point, the YAMNL descriptor can also be stored in some git repo and used as a template for future installations. You use the Tanzu CLI to start the web UI tanzu mc create --ui which by default will listen on 127.0.0.1:8080 . If you run it from the bastion host, you can either use the --bind flag or you need to create a ssh tunnel to the bastion like ssh -l <BASTION-USER> -L 8080 :localhost:8080 <BASTION-HOST-IP-OR-FQDN> and then direct your browser to http://localhost:8080 . Now you can go through step-by-step process, providing the required information (i.e. vCenter endpoint and credentials, cluster details, ...). In the end, you review your configuration and are able to go back to change some bits; when everything looks good, you can go ahead and deploy the management cluster directly or export the generated YAML descriptor. Do make sure you export the file and go through it to get yourself comfortable with all the settings. Also, if you need to set up OIDC authentication, please take a look at the identity management document . Prepare the tools \u00b6 If you are planning to run TKGm in an isolated environment, you should have already relocated all the images , and now you need to make the Tanzu CLI aware of your internal registry. You can do so by setting the following variables: TKG_CUSTOM_IMAGE_REPOSITORY stores the base URL of the images, which is <REGISTRY>/<PROJECT> TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE stores the base64-encoded certificate of the CA that trusts the registry's certificate Warning The value of TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE MUST NOT contain newline characters. These values can be set either as environment variables or Tanzu CLI's variables. Tanzu CLI variables (recommended) Environment variables You add the variables to the Tanzu CLI configuration, which is stored in a file thus persisted across different shells. This is the preferred method when your bastion host is used to manage only one management cluster or registry. tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY \"<REGISTRY-FQDN>/tkg\" tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE \" $( base64 -w0 /path/to/harbor-ca.pem ) \" This is the preferred method if you have to deal with multiple environments with different configurations (i.e. different OCI registries), which is not this case. export TKG_CUSTOM_IMAGE_REPOSITORY = \"<REGISTRY-FQDN>/tkg\" export TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE = \" $( base64 -w0 < /path/to/harbor-ca.pem ) \" Do make sure your Docker client trusts the registry as explained in the dedicated section . Deploy the management cluster \u00b6 It's finally time to build the management cluster. You need to add one more bit to your configuration file, otherwise the Tanzu CLI will prompt you for confirmation about deploying a management cluster on top of vSphere rather than an integrated vSphere with Tanzu (Tanzu Kubernetes Grid service). DEPLOY_TKG_ON_VSPHERE7 : true Now you can create your management cluster as tanzu mc create --file /path/to/the/yaml/config/file -v 6","title":"TKG management cluster"},{"location":"tkgm/management-cluster/#tkg-management-cluster","text":"Info The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-index.html .","title":"TKG management cluster"},{"location":"tkgm/management-cluster/#import-node-os-images","text":"TKGm can be run either on Ubuntu or Photon VMs, whose base OS images are available on Customer Connect. \u276f vcc get versions -p vmware_tanzu_kubernetes_grid -s tkg ... FILENAME SIZE BUILD NUMBER DESCRIPTION ... photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova 1 .02 GB 21158328 Photon v3 Kubernetes v1.24.9 OVA photon-3-kube-v1.23.15+vmware.1-tkg.1-2d8c52b5e3dc2b7a03bf3a04815b9474.ova 1022 .45 MB 21158328 Photon v3 Kubernetes v1.23.15 OVA photon-3-kube-v1.22.17+vmware.1-tkg.1-a10ef8e088cc2c89418bca79a2fcc594.ova 1 GB 21158328 Photon v3 Kubernetes v1.22.17 OVA ubuntu-2004-kube-v1.24.9+vmware.1-tkg.1-b030088fe71fea7ff1ecb87a4d425c93.ova 1 .78 GB 21158328 Ubuntu 2004 Kubernetes v1.24.9 OVA ubuntu-2004-kube-v1.23.15+vmware.1-tkg.1-66940e9f3a5eda8bbd51042b1aaea6e1.ova 1 .74 GB 21158328 Ubuntu 2004 Kubernetes v1.23.15 OVA ubuntu-2004-kube-v1.22.17+vmware.1-tkg.1-df08b304658a6cf17f5e74dc0ab7543c.ova 1 .77 GB 21158328 Ubuntu 2004 Kubernetes v1.22.17 OVA ... You might want to select the latest version of a particular flavour, to benefit from the latest updates and getting closer to the most recent upstream version. In this case, we're going to pick the Photon image for Kubernetes v1.24.9 vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 -f 'photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova' --accepteula and import it to our vSphere content library, to make sure it's available when needed. govc library.import -n photon-1.24.9 ova $HOME /vcc-downloads/photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova However, TKGm expects a VM template to be available, therefore we need to deploy a template from this content library item. We dump the specs into a JSON file govc import.spec $HOME /vcc-downloads/photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova > photon.json and we edit it to make sure that it is marked as a template it does not get powered on the DiskProvisioning configuration is correct The result looks like this { \"DiskProvisioning\" : \"thin\" , \"IPAllocationPolicy\" : \"dhcpPolicy\" , \"IPProtocol\" : \"IPv4\" , \"NetworkMapping\" : [ { \"Name\" : \"nic0\" , \"Network\" : \"\" } ], \"Annotation\" : \"Cluster API vSphere image - VMware Photon OS 64-bit and Kubernetes v1.24.9+vmware.1 - https://github.com/kubernetes-sigs/cluster-api-provider-vsphere\" , \"MarkAsTemplate\" : true , \"PowerOn\" : false , \"InjectOvfEnv\" : false , \"WaitForIP\" : false , \"Name\" : null } Finally, we can deploy the OVA into a folder dedicated to our cluster govc folder.create /vc01/vm/tkgm govc library.deploy -options photon.json -folder tkgm /ova/photon-1.24.9","title":"Import node OS images"},{"location":"tkgm/management-cluster/#prepare-management-cluster-configuration","text":"Tanzu Kubernetes Grid standalone cluster deployment can be run in unattended mode via the Tanzu CLI, feeding it with a YAML configuration file . Alternatively, a web UI is also available for guiding the user through all the required settings. There is actually a third option, which is preferrable if you are not familiar with or not entirely sure about all the different settings available: it consists of using the web UI for generating the initial YAML file, and then fine tuning it, if needed, before submitting it to the Tanzu CLI. At this point, the YAMNL descriptor can also be stored in some git repo and used as a template for future installations. You use the Tanzu CLI to start the web UI tanzu mc create --ui which by default will listen on 127.0.0.1:8080 . If you run it from the bastion host, you can either use the --bind flag or you need to create a ssh tunnel to the bastion like ssh -l <BASTION-USER> -L 8080 :localhost:8080 <BASTION-HOST-IP-OR-FQDN> and then direct your browser to http://localhost:8080 . Now you can go through step-by-step process, providing the required information (i.e. vCenter endpoint and credentials, cluster details, ...). In the end, you review your configuration and are able to go back to change some bits; when everything looks good, you can go ahead and deploy the management cluster directly or export the generated YAML descriptor. Do make sure you export the file and go through it to get yourself comfortable with all the settings. Also, if you need to set up OIDC authentication, please take a look at the identity management document .","title":"Prepare management cluster configuration"},{"location":"tkgm/management-cluster/#prepare-the-tools","text":"If you are planning to run TKGm in an isolated environment, you should have already relocated all the images , and now you need to make the Tanzu CLI aware of your internal registry. You can do so by setting the following variables: TKG_CUSTOM_IMAGE_REPOSITORY stores the base URL of the images, which is <REGISTRY>/<PROJECT> TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE stores the base64-encoded certificate of the CA that trusts the registry's certificate Warning The value of TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE MUST NOT contain newline characters. These values can be set either as environment variables or Tanzu CLI's variables. Tanzu CLI variables (recommended) Environment variables You add the variables to the Tanzu CLI configuration, which is stored in a file thus persisted across different shells. This is the preferred method when your bastion host is used to manage only one management cluster or registry. tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY \"<REGISTRY-FQDN>/tkg\" tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE \" $( base64 -w0 /path/to/harbor-ca.pem ) \" This is the preferred method if you have to deal with multiple environments with different configurations (i.e. different OCI registries), which is not this case. export TKG_CUSTOM_IMAGE_REPOSITORY = \"<REGISTRY-FQDN>/tkg\" export TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE = \" $( base64 -w0 < /path/to/harbor-ca.pem ) \" Do make sure your Docker client trusts the registry as explained in the dedicated section .","title":"Prepare the tools"},{"location":"tkgm/management-cluster/#deploy-the-management-cluster","text":"It's finally time to build the management cluster. You need to add one more bit to your configuration file, otherwise the Tanzu CLI will prompt you for confirmation about deploying a management cluster on top of vSphere rather than an integrated vSphere with Tanzu (Tanzu Kubernetes Grid service). DEPLOY_TKG_ON_VSPHERE7 : true Now you can create your management cluster as tanzu mc create --file /path/to/the/yaml/config/file -v 6","title":"Deploy the management cluster"},{"location":"tkgm/monitoring/","text":"Monitoring \u00b6 metrics-server \u00b6 Relocate the metrics-server image RELEASE = \"v0.6.2\" imgpkg copy -i k8s.gcr.io/metrics-server/metrics-server: ${ RELEASE } --to-repo registry.h2o-2-6241.h2o.vmware.com/apps/metrics-server --registry-ca-cert-path harbor-ca.pem Download manifest and update image location: curl -sSfL https://github.com/kubernetes-sigs/metrics-server/releases/download/ ${ RELEASE } /high-availability.yaml -o metrics-server.yaml yq -i ea '.|=select(.kind == \"Deployment\" and .metadata.name == \"metrics-server\").spec.template.spec.containers[]|=select(.name == \"metrics-server\").image=\"registry.h2o-2-6241.h2o.vmware.com/apps/metrics-server:\"+env(RELEASE)' metrics-server.yaml Optionally move the metrics-server.yaml file to a bastion host in the airgapped environment and apply it: kubectl apply -f metrics-server.yaml kube-state-metrics \u00b6 https://chrisedrego.medium.com/kubernetes-monitoring-kube-state-metrics-df6546aea324 Relocate the kube-state-metrics image RELEASE = \"v2.8.1\" imgpkg copy -i registry.k8s.io/kube-state-metrics/kube-state-metrics: ${ RELEASE } --to-repo registry.h2o-2-6241.h2o.vmware.com/apps/kube-state-metrics --registry-ca-cert-path harbor-ca.pem Get the manifests and update image location: git -b $RELEASE --depth 1 clone https://github.com/kubernetes/kube-state-metrics ytt -f kube-state-metrics/examples/standard/ | yq ea '.|=select(.kind == \"Deployment\" and .metadata.name == \"kube-state-metrics\").spec.template.spec.containers[]|=select(.name == \"kube-state-metrics\").image=\"registry.h2o-2-6241.h2o.vmware.com/apps/kube-state-metrics:\"+env(RELEASE)' > kube-state-metrics.yaml Optionally move the kube-state-metrics.yaml file to a bastion host in the airgapped environment and apply it: kubectl apply -f kube-state-metrics.yaml","title":"Monitoring"},{"location":"tkgm/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"tkgm/monitoring/#metrics-server","text":"Relocate the metrics-server image RELEASE = \"v0.6.2\" imgpkg copy -i k8s.gcr.io/metrics-server/metrics-server: ${ RELEASE } --to-repo registry.h2o-2-6241.h2o.vmware.com/apps/metrics-server --registry-ca-cert-path harbor-ca.pem Download manifest and update image location: curl -sSfL https://github.com/kubernetes-sigs/metrics-server/releases/download/ ${ RELEASE } /high-availability.yaml -o metrics-server.yaml yq -i ea '.|=select(.kind == \"Deployment\" and .metadata.name == \"metrics-server\").spec.template.spec.containers[]|=select(.name == \"metrics-server\").image=\"registry.h2o-2-6241.h2o.vmware.com/apps/metrics-server:\"+env(RELEASE)' metrics-server.yaml Optionally move the metrics-server.yaml file to a bastion host in the airgapped environment and apply it: kubectl apply -f metrics-server.yaml","title":"metrics-server"},{"location":"tkgm/monitoring/#kube-state-metrics","text":"https://chrisedrego.medium.com/kubernetes-monitoring-kube-state-metrics-df6546aea324 Relocate the kube-state-metrics image RELEASE = \"v2.8.1\" imgpkg copy -i registry.k8s.io/kube-state-metrics/kube-state-metrics: ${ RELEASE } --to-repo registry.h2o-2-6241.h2o.vmware.com/apps/kube-state-metrics --registry-ca-cert-path harbor-ca.pem Get the manifests and update image location: git -b $RELEASE --depth 1 clone https://github.com/kubernetes/kube-state-metrics ytt -f kube-state-metrics/examples/standard/ | yq ea '.|=select(.kind == \"Deployment\" and .metadata.name == \"kube-state-metrics\").spec.template.spec.containers[]|=select(.name == \"kube-state-metrics\").image=\"registry.h2o-2-6241.h2o.vmware.com/apps/kube-state-metrics:\"+env(RELEASE)' > kube-state-metrics.yaml Optionally move the kube-state-metrics.yaml file to a bastion host in the airgapped environment and apply it: kubectl apply -f kube-state-metrics.yaml","title":"kube-state-metrics"},{"location":"tkgm/registry/","text":"Deployment and configuration \u00b6 A very common scenario for customers adopting Tanzu Kubernetes Grid platform is running it within an isolated environment, be it either just Internet-restricted or even air-gapped. To be able to tackle such situations, it is necessary to fulfil a few tasks and meet some prerequisites. Tanzu Kubernetes Grid images, as well as applications', must be available from within the isolated environment, thus it's necessary to fetch and make them available in advance to the platform via an internal OCI registry. VMware supports CNCF's Harbor , and, since TKGM 2.1.0, provides a ready-to-use OVA template, downloadable from Customer Connect . Download template \u00b6 After you've downloaded vcc and authenticated as described in the bastion host preparation guide , you can fetch the Harbor OVA: First of all, list the files available for tkg 2.1.0 vcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 and find the Harbor OVA file: FILENAME SIZE BUILD NUMBER DESCRIPTION ... photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova 1.27 GB Photon v4 Harbor v2.6.3 OVA ... Then download the file: vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 -f 'photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova' --accepteula The file will be stored by default at $HOME/vcc-downloads/ , and you can then upload it to a content library of choice on your vSphere platform. Make sure that govc is installed and authentication has been properly configured, like: export GOVC_URL = my.vcenter.fqdn export GOVC_USERNAME = my.vcenter.username export GOVC_PASSWORD = my.vcenter.password export GOVC_INSECURE = true The last line is needed if the vCenter certificate is not yet trusted by your bastion host. Now you can create a content library if you don't have one ready govc library.create ova and upload the Harbor OVA file govc library.import -n harbor ova $HOME /vcc-downloads/photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova Install appliance \u00b6 The template is now available on vSphere and can be used to deploy a VM, which can be customised by providing values to the parameters that have been set in the OVA. This blog post explains how to make use of govc to deploy a VM from an OVA. Although it is about deploying a vCenter appliance, it is also generic enough to provide guidance for our case, too. Inspecting the OVA allows you to get all the properties that you need to provide a value for: govc import.spec $HOME /vcc-downloads/photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova | jq > harbor.spec.json and you get something like the following sample JSON snippet harbor.json { \"DiskProvisioning\" : \"flat\" , \"IPAllocationPolicy\" : \"dhcpPolicy\" , \"IPProtocol\" : \"IPv4\" , \"PropertyMapping\" : [{ \"Key\" : \"guestinfo.root_password\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.allow_root_ssh\" , \"Value\" : \"True\" }, { \"Key\" : \"guestinfo.harbor_hostname\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_admin_password\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_database_password\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_scanner_enable\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_selfsigned_cert\" , \"Value\" : \"True\" }, { \"Key\" : \"guestinfo.harbor_ca\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_server_cert\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_server_key\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_ip_address\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_netmask\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_gateway\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_dns_server\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_dns_domain\" , \"Value\" : \"\" } ], \"NetworkMapping\" : [{ \"Name\" : \"nic0\" , \"Network\" : \"\" }], \"Annotation\" : \"Harbor ova vSphere image - VMware Photon OS 64-bit and Harbor v2.6.3+vmware.1\" , \"MarkAsTemplate\" : false , \"PowerOn\" : false , \"InjectOvfEnv\" : false , \"WaitForIP\" : false , \"Name\" : null } which you can tune to your liking by filling in the actual values for your environment, and supply to the following command to deploy a new virtual machine (named registry ) running Harbor: govc library.deploy -options harbor.json /ova/harbor registry Tip DiskProvisioning Make sure you do specify the correct value for DiskProvisioning (i.e. thin ), or you may hit an error like govc: deploy error: Invalid value for DatastoreMappingParams.target_provisioning_type: flat. Harbor hostname Set the guestinfo.harbor_hostname property to the fully-qualified domain name you want to use to reach the service. If the certificate gets auto-generated, this is the value that is used as Common Name (and SAN, too), otherwise, it must match with the certificate you provide. Harbor scanner Set the guestinfo.harbor_scanner_enable property to either \"True\" or \"False\" (as a string), to avoid the following error at power on: govc: Property 'Enable Harbor Default Scanner' must be configured for the VM to power on. Certificates and keys CA and server certificates and key must be supplied in plain PEM format, not base64-encoded. If no values are provided they will be auto-generated, thus they will be self-signed (not recommended for production use). Trust your registry \u00b6 You need to make sure that the Docker client on the bastion host trusts the internal registry. In fact, when you will get to deploy Tanzu Kubernetes Grid, the Tanzu CLI will use the Docker client to run a local kind cluster, that will eventually deploy all the required components to the actual location (i.e. your vSphere cluster). The Docker client will attempt to pull the kind image from the internal registry, and will ultimately fail if the trust is not established. Docker verifies a registry certificate against system CAs and additional CA certificates at /etc/docker/certs.d/<REGISTRY-FQDN>/ca.crt . If the URL of the registry you want to interact with has a port you need to specify it as well. See Docker docs for more information. So you need to copy the CA certificate into the above location as sudo mkdir /etc/docker/certs.d/<REGISTRY-FQDN> sudo cp /path/to/harbor-ca.pem /etc/docker/certs.d/<REGISTRY-FQDN>/ca.crt sudo systemctl reload docker Configure tkg project \u00b6 Tanzu Kubernetes Grid images must be loaded into a dedicated project in Harbor, a hierarchical object that acts as a folder and allows to set specific configurations in terms of, e.g., permissions and quota. Navigating to the Harbor UI 1 as admin user, you can follow the Projects link, and create a new project hitting the + NEW PROJECT button. The pop-up modal window allows you to set the name (we can name it tkg ), the quota and whether or not it should be publicly accessible (no authentication required). More granular settings can be applied by editing the project after it has been created. If the organisation's security policies allow, it is simpler to set this project as public, because images are publicly available anyways at projects.registry.vmware.com/tkg public access means that unauthorised users (which includes the unauthenticated ones) are allowed to pull but not to push images However, if stricter policies apply, a read-only user will need to be created during next step . Configure users \u00b6 You now need to create a user with write permissions to the tkg project created previously . You can do so by navigating to the Administration > Users menu in the Harbor UI 1 . Once the user has been created, it needs to be authorised to the tkg project, granting it at least the developer role: go back to Projects , click on the tkg project to edit it and add a user selecting the correct role. More info on roles at the official Harbor docs . in the future, I might want to add a section to this guide for configuring Harbor programmatically, either by curl ing APIs, or leveraging tools like Terraform or Ansible. \u21a9 \u21a9","title":"Deployment and configuration"},{"location":"tkgm/registry/#deployment-and-configuration","text":"A very common scenario for customers adopting Tanzu Kubernetes Grid platform is running it within an isolated environment, be it either just Internet-restricted or even air-gapped. To be able to tackle such situations, it is necessary to fulfil a few tasks and meet some prerequisites. Tanzu Kubernetes Grid images, as well as applications', must be available from within the isolated environment, thus it's necessary to fetch and make them available in advance to the platform via an internal OCI registry. VMware supports CNCF's Harbor , and, since TKGM 2.1.0, provides a ready-to-use OVA template, downloadable from Customer Connect .","title":"Deployment and configuration"},{"location":"tkgm/registry/#download-template","text":"After you've downloaded vcc and authenticated as described in the bastion host preparation guide , you can fetch the Harbor OVA: First of all, list the files available for tkg 2.1.0 vcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 and find the Harbor OVA file: FILENAME SIZE BUILD NUMBER DESCRIPTION ... photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova 1.27 GB Photon v4 Harbor v2.6.3 OVA ... Then download the file: vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2 .1.0 -f 'photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova' --accepteula The file will be stored by default at $HOME/vcc-downloads/ , and you can then upload it to a content library of choice on your vSphere platform. Make sure that govc is installed and authentication has been properly configured, like: export GOVC_URL = my.vcenter.fqdn export GOVC_USERNAME = my.vcenter.username export GOVC_PASSWORD = my.vcenter.password export GOVC_INSECURE = true The last line is needed if the vCenter certificate is not yet trusted by your bastion host. Now you can create a content library if you don't have one ready govc library.create ova and upload the Harbor OVA file govc library.import -n harbor ova $HOME /vcc-downloads/photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova","title":"Download template"},{"location":"tkgm/registry/#install-appliance","text":"The template is now available on vSphere and can be used to deploy a VM, which can be customised by providing values to the parameters that have been set in the OVA. This blog post explains how to make use of govc to deploy a VM from an OVA. Although it is about deploying a vCenter appliance, it is also generic enough to provide guidance for our case, too. Inspecting the OVA allows you to get all the properties that you need to provide a value for: govc import.spec $HOME /vcc-downloads/photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova | jq > harbor.spec.json and you get something like the following sample JSON snippet harbor.json { \"DiskProvisioning\" : \"flat\" , \"IPAllocationPolicy\" : \"dhcpPolicy\" , \"IPProtocol\" : \"IPv4\" , \"PropertyMapping\" : [{ \"Key\" : \"guestinfo.root_password\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.allow_root_ssh\" , \"Value\" : \"True\" }, { \"Key\" : \"guestinfo.harbor_hostname\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_admin_password\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_database_password\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_scanner_enable\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_selfsigned_cert\" , \"Value\" : \"True\" }, { \"Key\" : \"guestinfo.harbor_ca\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_server_cert\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.harbor_server_key\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_ip_address\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_netmask\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_gateway\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_dns_server\" , \"Value\" : \"\" }, { \"Key\" : \"guestinfo.network_dns_domain\" , \"Value\" : \"\" } ], \"NetworkMapping\" : [{ \"Name\" : \"nic0\" , \"Network\" : \"\" }], \"Annotation\" : \"Harbor ova vSphere image - VMware Photon OS 64-bit and Harbor v2.6.3+vmware.1\" , \"MarkAsTemplate\" : false , \"PowerOn\" : false , \"InjectOvfEnv\" : false , \"WaitForIP\" : false , \"Name\" : null } which you can tune to your liking by filling in the actual values for your environment, and supply to the following command to deploy a new virtual machine (named registry ) running Harbor: govc library.deploy -options harbor.json /ova/harbor registry Tip DiskProvisioning Make sure you do specify the correct value for DiskProvisioning (i.e. thin ), or you may hit an error like govc: deploy error: Invalid value for DatastoreMappingParams.target_provisioning_type: flat. Harbor hostname Set the guestinfo.harbor_hostname property to the fully-qualified domain name you want to use to reach the service. If the certificate gets auto-generated, this is the value that is used as Common Name (and SAN, too), otherwise, it must match with the certificate you provide. Harbor scanner Set the guestinfo.harbor_scanner_enable property to either \"True\" or \"False\" (as a string), to avoid the following error at power on: govc: Property 'Enable Harbor Default Scanner' must be configured for the VM to power on. Certificates and keys CA and server certificates and key must be supplied in plain PEM format, not base64-encoded. If no values are provided they will be auto-generated, thus they will be self-signed (not recommended for production use).","title":"Install appliance"},{"location":"tkgm/registry/#trust-your-registry","text":"You need to make sure that the Docker client on the bastion host trusts the internal registry. In fact, when you will get to deploy Tanzu Kubernetes Grid, the Tanzu CLI will use the Docker client to run a local kind cluster, that will eventually deploy all the required components to the actual location (i.e. your vSphere cluster). The Docker client will attempt to pull the kind image from the internal registry, and will ultimately fail if the trust is not established. Docker verifies a registry certificate against system CAs and additional CA certificates at /etc/docker/certs.d/<REGISTRY-FQDN>/ca.crt . If the URL of the registry you want to interact with has a port you need to specify it as well. See Docker docs for more information. So you need to copy the CA certificate into the above location as sudo mkdir /etc/docker/certs.d/<REGISTRY-FQDN> sudo cp /path/to/harbor-ca.pem /etc/docker/certs.d/<REGISTRY-FQDN>/ca.crt sudo systemctl reload docker","title":"Trust your registry"},{"location":"tkgm/registry/#configure-tkg-project","text":"Tanzu Kubernetes Grid images must be loaded into a dedicated project in Harbor, a hierarchical object that acts as a folder and allows to set specific configurations in terms of, e.g., permissions and quota. Navigating to the Harbor UI 1 as admin user, you can follow the Projects link, and create a new project hitting the + NEW PROJECT button. The pop-up modal window allows you to set the name (we can name it tkg ), the quota and whether or not it should be publicly accessible (no authentication required). More granular settings can be applied by editing the project after it has been created. If the organisation's security policies allow, it is simpler to set this project as public, because images are publicly available anyways at projects.registry.vmware.com/tkg public access means that unauthorised users (which includes the unauthenticated ones) are allowed to pull but not to push images However, if stricter policies apply, a read-only user will need to be created during next step .","title":"Configure tkg project"},{"location":"tkgm/registry/#configure-users","text":"You now need to create a user with write permissions to the tkg project created previously . You can do so by navigating to the Administration > Users menu in the Harbor UI 1 . Once the user has been created, it needs to be authorised to the tkg project, granting it at least the developer role: go back to Projects , click on the tkg project to edit it and add a user selecting the correct role. More info on roles at the official Harbor docs . in the future, I might want to add a section to this guide for configuring Harbor programmatically, either by curl ing APIs, or leveraging tools like Terraform or Ansible. \u21a9 \u21a9","title":"Configure users"},{"location":"tkgm/relocate-images/","text":"Relocate TKG images \u00b6 Tanzu Kubernetes Grid images are available for download at projects.registry.vmware.com , and can be copied to an internal registry for serving air-gapped or Internet-restricted installations. The Tanzu CLI comes with an handy plugin that allows you to relocate TKGm images, as better described in TKGm docs . First of all, you need to download all the required images to your local file system mkdir -p tkgm-bundle && cd tkgm-bundle tanzu isolated-cluster download-bundle \\ --source-repo projects.registry.vmware.com/tkg \\ --tkg-version v2.1.0 you copy the entire tkgm-bundle directory into your air-gapped environment 1 , and then you can upload the images to your registry. The Tanzu CLI uses imgpkg under the hood, so you manage authentication as described in imgpkg docs . For example, you can use environment variables, like: export IMGPKG_REGISTRY_HOSTNAME_0 = '<REGISTRY-FQDN>' export IMGPKG_REGISTRY_USERNAME_0 = '<REGISTRY-USERNAME>' export IMGPKG_REGISTRY_PASSWORD_0 = '<REGISTRY-PASSWORD>' and then you can upload the images using the tanzu isolated-cluster upload-bundle command. The username and password have been set during Harbor preparation . When using Harbor as a registry, you can download the CA certificate to a file via curl -sSfLk -H 'accept: application/octet-stream' -o harbor-ca.pem https://<REGISTRY-FQDN>/api/v2.0/systeminfo/getcert and then push the images tanzu isolated-cluster upload-bundle \\ --source-directory tkgm-bundle \\ --destination-repo \"<REGISTRY-FQDN>/tkg\" \\ --ca-certificate harbor-ca.pem Note Before uploading the images, make sure you create a project named tkg , as described in the previous section , and grant write permissions to <REGISTRY-USERNAME> . if you plan to do an air-gapped TKGm installation, your bastion host does not have access to the internal registry, therefore you need to copy the images to another host that does, which is tipically inside the isolated environment \u21a9","title":"Relocate TKG images"},{"location":"tkgm/relocate-images/#relocate-tkg-images","text":"Tanzu Kubernetes Grid images are available for download at projects.registry.vmware.com , and can be copied to an internal registry for serving air-gapped or Internet-restricted installations. The Tanzu CLI comes with an handy plugin that allows you to relocate TKGm images, as better described in TKGm docs . First of all, you need to download all the required images to your local file system mkdir -p tkgm-bundle && cd tkgm-bundle tanzu isolated-cluster download-bundle \\ --source-repo projects.registry.vmware.com/tkg \\ --tkg-version v2.1.0 you copy the entire tkgm-bundle directory into your air-gapped environment 1 , and then you can upload the images to your registry. The Tanzu CLI uses imgpkg under the hood, so you manage authentication as described in imgpkg docs . For example, you can use environment variables, like: export IMGPKG_REGISTRY_HOSTNAME_0 = '<REGISTRY-FQDN>' export IMGPKG_REGISTRY_USERNAME_0 = '<REGISTRY-USERNAME>' export IMGPKG_REGISTRY_PASSWORD_0 = '<REGISTRY-PASSWORD>' and then you can upload the images using the tanzu isolated-cluster upload-bundle command. The username and password have been set during Harbor preparation . When using Harbor as a registry, you can download the CA certificate to a file via curl -sSfLk -H 'accept: application/octet-stream' -o harbor-ca.pem https://<REGISTRY-FQDN>/api/v2.0/systeminfo/getcert and then push the images tanzu isolated-cluster upload-bundle \\ --source-directory tkgm-bundle \\ --destination-repo \"<REGISTRY-FQDN>/tkg\" \\ --ca-certificate harbor-ca.pem Note Before uploading the images, make sure you create a project named tkg , as described in the previous section , and grant write permissions to <REGISTRY-USERNAME> . if you plan to do an air-gapped TKGm installation, your bastion host does not have access to the internal registry, therefore you need to copy the images to another host that does, which is tipically inside the isolated environment \u21a9","title":"Relocate TKG images"},{"location":"tkgm/troubleshooting/","text":"Troubleshooting \u00b6 SSH into the nodes \u00b6 Nodes not joining the cluster \u00b6 Change Kube-VIP IP address \u00b6 You should never need to change the IP address of the Kube-VIP load balancer, because it is something you should carefully plan and decide upfront; but in case you do need it, this section may be helpful. Warning This is a quite impactful operation, as it may (and will) break the connections between all the nodes. In the end you might also need to roll all the worker nodes out, for the change to be fully propagated. So, depending on your cluster size, get yourself a maintenance window long enough to rebuild all the nodes. Do remember that you have to pick an address belonging to the same control-plane nodes network, and make sure it is outside the DHCP range. Kube-VIP is managed via a static pod running on the control-plane nodes, this means that its definition lies on a plain YAML file on the node's file system. For kubeadm-based clusters, the static pods manifest files are stored in the /etc/kubernetes/manifests directory, and TKG is no exception. Log into the control-plane nodes and go to the manifests directory. Print the definition of the Kube-VIP pod, if you wish: cd /etc/kubernetes/manifests cat kube-vip.yaml Now you need to edit the YAML file and set the value of the environment variable named address to the new IP address, like: apiVersion : v1 kind : Pod metadata : creationTimestamp : null name : kube-vip namespace : kube-system spec : containers : - args : - manager env : ... - name : address value : 172.31.8.12 ... Do the same thing on all the control-plane nodes of the cluster, and then delete all the kube-vip-* pods from the kube-system namespace, to get them recreated right away with the new configuration: kubectl -n kube-system get pods -o name | grep kube-vip- | xargs kubectl -n kube-system delete You also need to change the ConfigMap kubeadm-config in the kube-system namespace, to make sure that future nodes will get configured correctly. Set the key controlPlaneEndpoint in data.ClusterConfiguration to the new IP address, followed by :6443 (the standard kube-api port does not change). Add names or IPs to API server certificate \u00b6 If you want to reach out to your Kubernetes cluster using different DNS names and/or IP addresses (i.e. you need to change the Kube-VIP IP address ), you may also want to add those endpoints as SANs in the API server certificate. First of all, you need to log into the control-plane nodes via SSH and cd to the /etc/kubernetes/pki directory. Here you can find all the certificates and keys used by Kubernetes, and the one you care about is apiserver.crt . You can actually take a look at it and make sure it does not include the new endpoints, yet: sudo openssl x509 -text < apiserver.crt The kubeadm tool is used for the whole lifecycle of the cluster, and it comes in handy in this case, too. For example, you can check certificates expiration executing kubeadm certs check-expiration which shows you also which CA signed the certificates and whether it is external or not. Warning The following procedure applies when no external CA has been used to sign the API server certificate. Kubeadm stores its configuration in a ConfigMap in the cluster, and that's where you start from to add the new endpoints. Fetch the ConfigMap contents and store it to a file: sudo kubectl -n kube-system get cm kubeadm-config -o jsonpath = '{.data.ClusterConfiguration}' --kubeconfig /etc/kubernetes/admin.conf > /tmp/kubeadm.conf Then, edit the /tmp/kubeadm.conf file to add (or edit if already present) the apiServer.certSANs key, as follows: apiServer : certSANs : - 172.31.8.12 - my.cluster.endpoint.fqdn Make sure you do remove the old certificate and key, otherwise kubeadm will not create a new one. Store them somewhere else rather than deleting them, just in case: sudo mv /etc/kubernetes/pki/apiserver. { crt,key } ~ Now run the command to build a new certificate and key pair: sudo kubeadm init phase certs apiserver --config /tmp/kubeadm.conf The API server picks up the new certificate immediately and you can assert it includes the new SANs . Warning If the control-plane is composed of multiple nodes, do make sure you repeat the same process on all of them. DO NOT copy the certificate across all the nodes, as its SANs include also the IP address of the node itself, which is unique for each node. However, you can feed the same kubeadm.conf file to the previous kubeadm command on all nodes. Finally, upload the modified kubeadm configuration back to the ConfigMap in the cluster kubeadm init phase upload-config kubeadm --config /tmp/kubeadm.conf --kubeconfig /etc/kubernetes/admin.conf Change cluster configuration \u00b6 The Cluster objects represent the actual Tanzu clusters known to the management cluster, both the management itself and the workload clusters. A change to those object forces all the nodes of the cluster (control-plane and worker nodes) to be rolled out with the new configuration. For example, if you need to apply or change the proxy settings to the cluster, you can add/change an item, similar to the following, to the spec.topology.variables list: - name : proxy value : httpProxy : http://proxy.my.domain:proxy-port httpsProxy : http://proxy.my.domain:proxy-port noProxy : - .my.domain - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 - localhost - 127.0.0.1 - .svc - .svc.cluster.local - 100.96.0.0/11 - 100.64.0.0/13 You can then observe the nodes as they get replaced. Warning Be aware that this will make the connection drop, as the Kube-VIP will need to be re-negotiated between the old and new node(s), so make sure you run this during a proper maintenance window. Provide custom CA certificates to Pinniped \u00b6 If your OIDC IdP endpoint exposes a TLS certificate signed by an unknown authority (i.e. own company CA), you need to provide the CA certificate to pinniped, otherwise the connection will fail. Same thing if pinniped gets to the IdP endpoint via a TLS-enabled proxy that uses a custom CA","title":"Troubleshooting"},{"location":"tkgm/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"tkgm/troubleshooting/#ssh-into-the-nodes","text":"","title":"SSH into the nodes"},{"location":"tkgm/troubleshooting/#nodes-not-joining-the-cluster","text":"","title":"Nodes not joining the cluster"},{"location":"tkgm/troubleshooting/#change-kube-vip-ip-address","text":"You should never need to change the IP address of the Kube-VIP load balancer, because it is something you should carefully plan and decide upfront; but in case you do need it, this section may be helpful. Warning This is a quite impactful operation, as it may (and will) break the connections between all the nodes. In the end you might also need to roll all the worker nodes out, for the change to be fully propagated. So, depending on your cluster size, get yourself a maintenance window long enough to rebuild all the nodes. Do remember that you have to pick an address belonging to the same control-plane nodes network, and make sure it is outside the DHCP range. Kube-VIP is managed via a static pod running on the control-plane nodes, this means that its definition lies on a plain YAML file on the node's file system. For kubeadm-based clusters, the static pods manifest files are stored in the /etc/kubernetes/manifests directory, and TKG is no exception. Log into the control-plane nodes and go to the manifests directory. Print the definition of the Kube-VIP pod, if you wish: cd /etc/kubernetes/manifests cat kube-vip.yaml Now you need to edit the YAML file and set the value of the environment variable named address to the new IP address, like: apiVersion : v1 kind : Pod metadata : creationTimestamp : null name : kube-vip namespace : kube-system spec : containers : - args : - manager env : ... - name : address value : 172.31.8.12 ... Do the same thing on all the control-plane nodes of the cluster, and then delete all the kube-vip-* pods from the kube-system namespace, to get them recreated right away with the new configuration: kubectl -n kube-system get pods -o name | grep kube-vip- | xargs kubectl -n kube-system delete You also need to change the ConfigMap kubeadm-config in the kube-system namespace, to make sure that future nodes will get configured correctly. Set the key controlPlaneEndpoint in data.ClusterConfiguration to the new IP address, followed by :6443 (the standard kube-api port does not change).","title":"Change Kube-VIP IP address"},{"location":"tkgm/troubleshooting/#add-names-or-ips-to-api-server-certificate","text":"If you want to reach out to your Kubernetes cluster using different DNS names and/or IP addresses (i.e. you need to change the Kube-VIP IP address ), you may also want to add those endpoints as SANs in the API server certificate. First of all, you need to log into the control-plane nodes via SSH and cd to the /etc/kubernetes/pki directory. Here you can find all the certificates and keys used by Kubernetes, and the one you care about is apiserver.crt . You can actually take a look at it and make sure it does not include the new endpoints, yet: sudo openssl x509 -text < apiserver.crt The kubeadm tool is used for the whole lifecycle of the cluster, and it comes in handy in this case, too. For example, you can check certificates expiration executing kubeadm certs check-expiration which shows you also which CA signed the certificates and whether it is external or not. Warning The following procedure applies when no external CA has been used to sign the API server certificate. Kubeadm stores its configuration in a ConfigMap in the cluster, and that's where you start from to add the new endpoints. Fetch the ConfigMap contents and store it to a file: sudo kubectl -n kube-system get cm kubeadm-config -o jsonpath = '{.data.ClusterConfiguration}' --kubeconfig /etc/kubernetes/admin.conf > /tmp/kubeadm.conf Then, edit the /tmp/kubeadm.conf file to add (or edit if already present) the apiServer.certSANs key, as follows: apiServer : certSANs : - 172.31.8.12 - my.cluster.endpoint.fqdn Make sure you do remove the old certificate and key, otherwise kubeadm will not create a new one. Store them somewhere else rather than deleting them, just in case: sudo mv /etc/kubernetes/pki/apiserver. { crt,key } ~ Now run the command to build a new certificate and key pair: sudo kubeadm init phase certs apiserver --config /tmp/kubeadm.conf The API server picks up the new certificate immediately and you can assert it includes the new SANs . Warning If the control-plane is composed of multiple nodes, do make sure you repeat the same process on all of them. DO NOT copy the certificate across all the nodes, as its SANs include also the IP address of the node itself, which is unique for each node. However, you can feed the same kubeadm.conf file to the previous kubeadm command on all nodes. Finally, upload the modified kubeadm configuration back to the ConfigMap in the cluster kubeadm init phase upload-config kubeadm --config /tmp/kubeadm.conf --kubeconfig /etc/kubernetes/admin.conf","title":"Add names or IPs to API server certificate"},{"location":"tkgm/troubleshooting/#change-cluster-configuration","text":"The Cluster objects represent the actual Tanzu clusters known to the management cluster, both the management itself and the workload clusters. A change to those object forces all the nodes of the cluster (control-plane and worker nodes) to be rolled out with the new configuration. For example, if you need to apply or change the proxy settings to the cluster, you can add/change an item, similar to the following, to the spec.topology.variables list: - name : proxy value : httpProxy : http://proxy.my.domain:proxy-port httpsProxy : http://proxy.my.domain:proxy-port noProxy : - .my.domain - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 - localhost - 127.0.0.1 - .svc - .svc.cluster.local - 100.96.0.0/11 - 100.64.0.0/13 You can then observe the nodes as they get replaced. Warning Be aware that this will make the connection drop, as the Kube-VIP will need to be re-negotiated between the old and new node(s), so make sure you run this during a proper maintenance window.","title":"Change cluster configuration"},{"location":"tkgm/troubleshooting/#provide-custom-ca-certificates-to-pinniped","text":"If your OIDC IdP endpoint exposes a TLS certificate signed by an unknown authority (i.e. own company CA), you need to provide the CA certificate to pinniped, otherwise the connection will fail. Same thing if pinniped gets to the IdP endpoint via a TLS-enabled proxy that uses a custom CA","title":"Provide custom CA certificates to Pinniped"},{"location":"tkgm/workload-cluster/","text":"TKG workload cluster \u00b6 Info The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/using-tkg-21/workload-index.html . Tanzu Kubernetes Grid workload clusters are the Kubernetes clusters responsible for running your applications, and can be created using the Tanzu CLI from a TKG management cluster on the same platform (i.e. vSphere). You can create three types of workload clusters: Class-based clusters (default) Plan-based clusters (legacy) TKC-based clusters (legacy) More information about the different classes are available on VMware docs website . This guide explains how to create class-based clusters. Prepare workload cluster definition \u00b6 The class-based workflow cluster manifest can be created from a flat file, similar to the one used for building the management cluster. Following VMware docs , you can create the flat configuration file, for example: AVI_CONTROL_PLANE_HA_PROVIDER : false CLUSTER_ANNOTATIONS : 'description:alpha-workload-cluster,location:non,mc:t1xl' CLUSTER_CIDR : 100.96.0.0/11 SERVICE_CIDR : 100.64.0.0/13 CLUSTER_PLAN : dev NAMESPACE : default CNI : antrea ENABLE_AUDIT_LOGGING : false ENABLE_CEIP_PARTICIPATION : false ENABLE_MHC : true ENABLE_MHC_CONTROL_PLANE : true ENABLE_MHC_WORKER_NODE : true MHC_UNKNOWN_STATUS_TIMEOUT : 5m MHC_FALSE_STATUS_TIMEOUT : 12m IDENTITY_MANAGEMENT_TYPE : none INFRASTRUCTURE_PROVIDER : vsphere OS_ARCH : amd64 OS_NAME : photon OS_VERSION : \"3\" TKG_HTTP_PROXY_ENABLED : false VSPHERE_CONTROL_PLANE_ENDPOINT : my.endpoint.fqdn.or.ip VSPHERE_DATACENTER : /vc01 VSPHERE_DATASTORE : /vc01/datastore/vsanDatastore VSPHERE_FOLDER : /vc01/vm/tkgm/alpha VSPHERE_INSECURE : false VSPHERE_NETWORK : /vc01/network/tkgm_vip_workload VSPHERE_PASSWORD : <encoded:bXktb3duLXBhc3N3b3JkCg==> VSPHERE_RESOURCE_POOL : /vc01/host/vc01cl01/Resources VSPHERE_SERVER : my.vcenter.fqdn.or.ip VSPHERE_SSH_AUTHORIZED_KEY : ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICAKfCTddh2+U/Y3DmhTwq9pqr+bWCZFxEOF1S2LVxsR VSPHERE_TLS_THUMBPRINT : B6:C1:C2:77:2F:E5:BA:C7:C1:C3:AD:59:E1:76:E4:19:13:75:30:ED VSPHERE_USERNAME : administrator@vsphere.local You can also use a different user with fewer privileges than administrator@vsphere.local for building the workload cluster, more info on this will be provided in a different section. All settings are detailed in the VMware docs website . You can get the vCenter SHA1 fingerprint (for the VSPHERE_TLS_THUMBPRINT setting) as openssl s_client -connect my.vcenter.fqdn.or.ip:443 -servername my.vcenter.fqdn.or.ip -showcerts </dev/null 2 >/dev/null | openssl x509 -fingerprint -noout -sha1 | cut -d = -f2 Warning The -sha1 flag is necessary especially on MacOS's openssl, where the default output digest is md5. You may now convert the flat file to the class-based file. First of all, set the Tanzu CLI to not apply automatically the generated class-based file: tanzu config set features.cluster.auto-apply-generated-clusterclass-based-configuration false then you can produce such file as tanzu cluster create alpha --file alpha-config.yaml --dry-run > alpha-spec.yaml This command takes also care of validating your inputs, therefore if something is not right (i.e. the designated vSphere folder does not exist) it will fail with a meaningful message. The generated YAML file defines these resource types: VSphereCPIConfig VSphereCSIConfig ClusterBootstrap Secret Cluster The ClusterBootstrap defines which components will be deployed to the new cluster as Tanzu packages. One thing to notice is that those components do not include the version, but they rather specify * apiVersion : run.tanzu.vmware.com/v1alpha3 kind : ClusterBootstrap metadata : annotations : tkg.tanzu.vmware.com/add-missing-fields-from-tkr : v1.24.9---vmware.1-tkg.1 name : alpha namespace : default spec : additionalPackages : - refName : metrics-server* - refName : secretgen-controller* - refName : pinniped* cpi : refName : vsphere-cpi* valuesFrom : providerRef : apiGroup : cpi.tanzu.vmware.com kind : VSphereCPIConfig name : alpha csi : refName : vsphere-csi* valuesFrom : providerRef : apiGroup : csi.tanzu.vmware.com kind : VSphereCSIConfig name : alpha kapp : refName : kapp-controller* to include any possible version that may have been made available to the cluster via a Tanzu package repository. The downside is that this resource cannot be used as a desired state and thus included into a GitOps-like process, as those * s are immediately translated into actual versions as soon as the workload cluster gets created. Create workload cluster \u00b6 You now have the YAML manifest for the new cluster, which must be applied to the management cluster. Make sure you do have your management cluster configuration and set it as your default kubeconfig file. You should be able to find it at ~/.kube-tkg/config , so you just have to set export KUBECONFIG = ~/.kube-tkg/config and you can check you're connected to the right endpoint via kubectl get nodes You can also retrieve the kubeconfig file via tanzu mc kubeconfig get --admin and switch to the right context. Now apply the workload cluster manifest: kubectl apply -f alpha-spec.yaml You can monitor the status of the resources running a kubectl get wrapped in a watch : watch kubectl get md,ma,vspherevm,vspheremachines and \u276f tanzu cluster list NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR alpha default running 1 /1 2 /2 v1.24.9+vmware.1 <none> dev v1.24.9---vmware.1-tkg.1 Connect to the new cluster \u00b6 Ask the Tanzu CLI to fetch the kubeconfig file for you and store it to a file. To get the configuration added to the currently active kubeconfig file you must omit the --export-file flag: \u276f tanzu cluster kubeconfig get alpha --admin --export-file alpha.kubeconfig Credentials of cluster 'alpha' have been saved You can now access the cluster by running 'kubectl config use-context alpha-admin@alpha' under path 'alpha.kubeconfig' Check the nodes status \u276f export KUBECONFIG = $( pwd ) /alpha.kubeconfig \u276f kubectl get nodes NAME STATUS ROLES AGE VERSION alpha-md-0-9qpjk-8694659b79-7mpxs Ready <none> 91s v1.24.9+vmware.1 alpha-md-0-9qpjk-8694659b79-vfrbk Ready <none> 89s v1.24.9+vmware.1 alpha-rv6pp-fkqg4 Ready control-plane 2m28s v1.24.9+vmware.1 You can now test the cluster with a sample nginx web server. As this environment is isolated, you need to push the image to the internal registry first (this step might be documented elsewhere in the future), and then you can run it. In this example, I created an apps project on the registry and pushed a nginx:stable-alpine to it. kubectl run --image registry.my.domain/apps/nginx:stable-alpine nginx kubectl port-forward pods/nginx 8080 :80 and, in another terminal, curl localhost:8080 the response confirms the pod is running correctly. <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > < style > html { color-scheme : light dark ; } body { width : 35 em ; margin : 0 auto ; font-family : Tahoma , Verdana , Arial , sans-serif ; } </ style > </ head > < body > < h1 > Welcome to nginx! </ h1 > < p > If you see this page, the nginx web server is successfully installed and working. Further configuration is required. </ p > < p > For online documentation and support please refer to < a href = \"http://nginx.org/\" > nginx.org </ a > . < br /> Commercial support is available at < a href = \"http://nginx.com/\" > nginx.com </ a > . </ p > < p >< em > Thank you for using nginx. </ em ></ p > </ body > </ html > Configure Tanzu standard repository \u00b6 TKG provides a number of packages available to be installed and used right out of the box, based on the Carvel kapp packaging system. They are delivered as part of a kapp repository, which is also available as OCI image, thats has been relocated in the internal registry during image relocation . You can add the repository as tanzu package repository add tanzu-standard --url registry.my.domain/tkg/packages/standard/repo:v2.1.0 --namespace tkg-system and, as soon as the reconciliation succeeds, you can list available packages: \u276f tanzu package available list NAME DISPLAY-NAME cert-manager.tanzu.vmware.com cert-manager contour.tanzu.vmware.com contour external-dns.tanzu.vmware.com external-dns fluent-bit.tanzu.vmware.com fluent-bit fluxcd-helm-controller.tanzu.vmware.com Flux Helm Controller fluxcd-kustomize-controller.tanzu.vmware.com Flux Kustomize Controller fluxcd-source-controller.tanzu.vmware.com Flux Source Controller grafana.tanzu.vmware.com grafana harbor.tanzu.vmware.com harbor multus-cni.tanzu.vmware.com multus-cni prometheus.tanzu.vmware.com prometheus whereabouts.tanzu.vmware.com whereabouts The packages are available in all namespaces, as the repository has been installed in kapp-controller's packaging global namespace.","title":"TKG workload cluster"},{"location":"tkgm/workload-cluster/#tkg-workload-cluster","text":"Info The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/using-tkg-21/workload-index.html . Tanzu Kubernetes Grid workload clusters are the Kubernetes clusters responsible for running your applications, and can be created using the Tanzu CLI from a TKG management cluster on the same platform (i.e. vSphere). You can create three types of workload clusters: Class-based clusters (default) Plan-based clusters (legacy) TKC-based clusters (legacy) More information about the different classes are available on VMware docs website . This guide explains how to create class-based clusters.","title":"TKG workload cluster"},{"location":"tkgm/workload-cluster/#prepare-workload-cluster-definition","text":"The class-based workflow cluster manifest can be created from a flat file, similar to the one used for building the management cluster. Following VMware docs , you can create the flat configuration file, for example: AVI_CONTROL_PLANE_HA_PROVIDER : false CLUSTER_ANNOTATIONS : 'description:alpha-workload-cluster,location:non,mc:t1xl' CLUSTER_CIDR : 100.96.0.0/11 SERVICE_CIDR : 100.64.0.0/13 CLUSTER_PLAN : dev NAMESPACE : default CNI : antrea ENABLE_AUDIT_LOGGING : false ENABLE_CEIP_PARTICIPATION : false ENABLE_MHC : true ENABLE_MHC_CONTROL_PLANE : true ENABLE_MHC_WORKER_NODE : true MHC_UNKNOWN_STATUS_TIMEOUT : 5m MHC_FALSE_STATUS_TIMEOUT : 12m IDENTITY_MANAGEMENT_TYPE : none INFRASTRUCTURE_PROVIDER : vsphere OS_ARCH : amd64 OS_NAME : photon OS_VERSION : \"3\" TKG_HTTP_PROXY_ENABLED : false VSPHERE_CONTROL_PLANE_ENDPOINT : my.endpoint.fqdn.or.ip VSPHERE_DATACENTER : /vc01 VSPHERE_DATASTORE : /vc01/datastore/vsanDatastore VSPHERE_FOLDER : /vc01/vm/tkgm/alpha VSPHERE_INSECURE : false VSPHERE_NETWORK : /vc01/network/tkgm_vip_workload VSPHERE_PASSWORD : <encoded:bXktb3duLXBhc3N3b3JkCg==> VSPHERE_RESOURCE_POOL : /vc01/host/vc01cl01/Resources VSPHERE_SERVER : my.vcenter.fqdn.or.ip VSPHERE_SSH_AUTHORIZED_KEY : ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICAKfCTddh2+U/Y3DmhTwq9pqr+bWCZFxEOF1S2LVxsR VSPHERE_TLS_THUMBPRINT : B6:C1:C2:77:2F:E5:BA:C7:C1:C3:AD:59:E1:76:E4:19:13:75:30:ED VSPHERE_USERNAME : administrator@vsphere.local You can also use a different user with fewer privileges than administrator@vsphere.local for building the workload cluster, more info on this will be provided in a different section. All settings are detailed in the VMware docs website . You can get the vCenter SHA1 fingerprint (for the VSPHERE_TLS_THUMBPRINT setting) as openssl s_client -connect my.vcenter.fqdn.or.ip:443 -servername my.vcenter.fqdn.or.ip -showcerts </dev/null 2 >/dev/null | openssl x509 -fingerprint -noout -sha1 | cut -d = -f2 Warning The -sha1 flag is necessary especially on MacOS's openssl, where the default output digest is md5. You may now convert the flat file to the class-based file. First of all, set the Tanzu CLI to not apply automatically the generated class-based file: tanzu config set features.cluster.auto-apply-generated-clusterclass-based-configuration false then you can produce such file as tanzu cluster create alpha --file alpha-config.yaml --dry-run > alpha-spec.yaml This command takes also care of validating your inputs, therefore if something is not right (i.e. the designated vSphere folder does not exist) it will fail with a meaningful message. The generated YAML file defines these resource types: VSphereCPIConfig VSphereCSIConfig ClusterBootstrap Secret Cluster The ClusterBootstrap defines which components will be deployed to the new cluster as Tanzu packages. One thing to notice is that those components do not include the version, but they rather specify * apiVersion : run.tanzu.vmware.com/v1alpha3 kind : ClusterBootstrap metadata : annotations : tkg.tanzu.vmware.com/add-missing-fields-from-tkr : v1.24.9---vmware.1-tkg.1 name : alpha namespace : default spec : additionalPackages : - refName : metrics-server* - refName : secretgen-controller* - refName : pinniped* cpi : refName : vsphere-cpi* valuesFrom : providerRef : apiGroup : cpi.tanzu.vmware.com kind : VSphereCPIConfig name : alpha csi : refName : vsphere-csi* valuesFrom : providerRef : apiGroup : csi.tanzu.vmware.com kind : VSphereCSIConfig name : alpha kapp : refName : kapp-controller* to include any possible version that may have been made available to the cluster via a Tanzu package repository. The downside is that this resource cannot be used as a desired state and thus included into a GitOps-like process, as those * s are immediately translated into actual versions as soon as the workload cluster gets created.","title":"Prepare workload cluster definition"},{"location":"tkgm/workload-cluster/#create-workload-cluster","text":"You now have the YAML manifest for the new cluster, which must be applied to the management cluster. Make sure you do have your management cluster configuration and set it as your default kubeconfig file. You should be able to find it at ~/.kube-tkg/config , so you just have to set export KUBECONFIG = ~/.kube-tkg/config and you can check you're connected to the right endpoint via kubectl get nodes You can also retrieve the kubeconfig file via tanzu mc kubeconfig get --admin and switch to the right context. Now apply the workload cluster manifest: kubectl apply -f alpha-spec.yaml You can monitor the status of the resources running a kubectl get wrapped in a watch : watch kubectl get md,ma,vspherevm,vspheremachines and \u276f tanzu cluster list NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR alpha default running 1 /1 2 /2 v1.24.9+vmware.1 <none> dev v1.24.9---vmware.1-tkg.1","title":"Create workload cluster"},{"location":"tkgm/workload-cluster/#connect-to-the-new-cluster","text":"Ask the Tanzu CLI to fetch the kubeconfig file for you and store it to a file. To get the configuration added to the currently active kubeconfig file you must omit the --export-file flag: \u276f tanzu cluster kubeconfig get alpha --admin --export-file alpha.kubeconfig Credentials of cluster 'alpha' have been saved You can now access the cluster by running 'kubectl config use-context alpha-admin@alpha' under path 'alpha.kubeconfig' Check the nodes status \u276f export KUBECONFIG = $( pwd ) /alpha.kubeconfig \u276f kubectl get nodes NAME STATUS ROLES AGE VERSION alpha-md-0-9qpjk-8694659b79-7mpxs Ready <none> 91s v1.24.9+vmware.1 alpha-md-0-9qpjk-8694659b79-vfrbk Ready <none> 89s v1.24.9+vmware.1 alpha-rv6pp-fkqg4 Ready control-plane 2m28s v1.24.9+vmware.1 You can now test the cluster with a sample nginx web server. As this environment is isolated, you need to push the image to the internal registry first (this step might be documented elsewhere in the future), and then you can run it. In this example, I created an apps project on the registry and pushed a nginx:stable-alpine to it. kubectl run --image registry.my.domain/apps/nginx:stable-alpine nginx kubectl port-forward pods/nginx 8080 :80 and, in another terminal, curl localhost:8080 the response confirms the pod is running correctly. <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > < style > html { color-scheme : light dark ; } body { width : 35 em ; margin : 0 auto ; font-family : Tahoma , Verdana , Arial , sans-serif ; } </ style > </ head > < body > < h1 > Welcome to nginx! </ h1 > < p > If you see this page, the nginx web server is successfully installed and working. Further configuration is required. </ p > < p > For online documentation and support please refer to < a href = \"http://nginx.org/\" > nginx.org </ a > . < br /> Commercial support is available at < a href = \"http://nginx.com/\" > nginx.com </ a > . </ p > < p >< em > Thank you for using nginx. </ em ></ p > </ body > </ html >","title":"Connect to the new cluster"},{"location":"tkgm/workload-cluster/#configure-tanzu-standard-repository","text":"TKG provides a number of packages available to be installed and used right out of the box, based on the Carvel kapp packaging system. They are delivered as part of a kapp repository, which is also available as OCI image, thats has been relocated in the internal registry during image relocation . You can add the repository as tanzu package repository add tanzu-standard --url registry.my.domain/tkg/packages/standard/repo:v2.1.0 --namespace tkg-system and, as soon as the reconciliation succeeds, you can list available packages: \u276f tanzu package available list NAME DISPLAY-NAME cert-manager.tanzu.vmware.com cert-manager contour.tanzu.vmware.com contour external-dns.tanzu.vmware.com external-dns fluent-bit.tanzu.vmware.com fluent-bit fluxcd-helm-controller.tanzu.vmware.com Flux Helm Controller fluxcd-kustomize-controller.tanzu.vmware.com Flux Kustomize Controller fluxcd-source-controller.tanzu.vmware.com Flux Source Controller grafana.tanzu.vmware.com grafana harbor.tanzu.vmware.com harbor multus-cni.tanzu.vmware.com multus-cni prometheus.tanzu.vmware.com prometheus whereabouts.tanzu.vmware.com whereabouts The packages are available in all namespaces, as the repository has been installed in kapp-controller's packaging global namespace.","title":"Configure Tanzu standard repository"}]}