{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Tanzu Gym","text":"<p>This web site is here for documenting my own experience and learnings around the VMware Tanzu portfolio.</p> <p>It's not meant to be a comprehensive source of information, but it's more a reference for the future me, in order not to forget what I've learnt along my Tanzu journey. However, I'll be more than happy if it will help others, too.</p> <p>If you feel like contributing or improving the repo with additional docs, fixes and suggestions, feel free to create an issue.</p>"},{"location":"tap/add-tap-repository/","title":"Add TAP kapp repository","text":"<p>You can now make your relocate images available to your cluster. Some environment variables used in this guide have already been defined in the relocate TAP images section, so do make sure you follow along.</p> <p>First of all, create a namespace where you plan to deploy TAP:</p> <pre><code>kubectl create ns tap-install || true\n</code></pre> <p>Then create a secret that will be used to authenticate to the registry and pull TAP images:</p> <pre><code>tanzu secret registry add tap-registry \\\n--server   ${IMGPKG_REGISTRY_HOSTNAME_1} \\\n--username ${IMGPKG_REGISTRY_USERNAME_1} \\\n--password ${IMGPKG_REGISTRY_PASSWORD_1} \\\n--namespace tap-install \\\n--export-to-all-namespaces \\\n--yes\n</code></pre> <p>Important</p> <p>Use a service account with read permissions to the <code>${TAP_REPOSITORY}</code> repository</p> <p>Add a Tanzu repository to make TAP packages available:</p> <pre><code>tanzu package repository add tanzu-tap-repository \\\n--url ${IMGPKG_REGISTRY_HOSTNAME_1}/${TAP_REPOSITORY}:${TAP_VERSION} \\\n--namespace tap-install\n</code></pre> <p>Verify that the repository has been reconciled successfully</p> <pre><code>\u276f tanzu package repository get tanzu-tap-repository --namespace tap-install\n\nNAMESPACE:               tap-install\nNAME:                    tanzu-tap-repository\nSOURCE:                  (imgpkg) registry.my.domain/tap/tap-packages:1.5.2\nSTATUS:                  Reconcile succeeded\nCONDITIONS:              - type: ReconcileSucceeded\n  status: \"True\"\nreason: \"\"\nmessage: \"\"\nUSEFUL-ERROR-MESSAGE:\n</code></pre> <p>and the packages are available</p> <pre><code>\u276f tanzu package available list --namespace tap-install\n\n  NAME                                                 DISPLAY-NAME\n  accelerator.apps.tanzu.vmware.com                    Application Accelerator for VMware Tanzu\n  api-portal.tanzu.vmware.com                          API portal\n  apis.apps.tanzu.vmware.com                           API Auto Registration for VMware Tanzu\n  apiserver.appliveview.tanzu.vmware.com               Application Live View ApiServer for VMware Tanzu\n  app-scanning.apps.tanzu.vmware.com                   Supply Chain Security Tools - App Scanning (Alpha)\napplication-configuration-service.tanzu.vmware.com   Application Configuration Service\n  backend.appliveview.tanzu.vmware.com                 Application Live View for VMware Tanzu\n  bitnami.services.tanzu.vmware.com                    bitnami-services\n  buildservice.tanzu.vmware.com                        Tanzu Build Service\n  carbonblack.scanning.apps.tanzu.vmware.com           VMware Carbon Black for Supply Chain Security Tools - Scan\n  cartographer.tanzu.vmware.com                        Cartographer\n  cnrs.tanzu.vmware.com                                Cloud Native Runtimes\n  connector.appliveview.tanzu.vmware.com               Application Live View Connector for VMware Tanzu\n  controller.source.apps.tanzu.vmware.com              Tanzu Source Controller\n  conventions.appliveview.tanzu.vmware.com             Application Live View Conventions for VMware Tanzu\n  crossplane.tanzu.vmware.com                          crossplane\n  developer-conventions.tanzu.vmware.com               Tanzu App Platform Developer Conventions\n  eventing.tanzu.vmware.com                            Eventing\n  external-secrets.apps.tanzu.vmware.com               External Secrets Operator\n  fluxcd.source.controller.tanzu.vmware.com            Flux Source Controller\n  grype.scanning.apps.tanzu.vmware.com                 Grype for Supply Chain Security Tools - Scan\n  learningcenter.tanzu.vmware.com                      Learning Center for Tanzu Application Platform\n  metadata-store.apps.tanzu.vmware.com                 Supply Chain Security Tools - Store\n  namespace-provisioner.apps.tanzu.vmware.com          Namespace Provisioner\n  ootb-delivery-basic.tanzu.vmware.com                 Tanzu App Platform Out of The Box Delivery Basic\n  ootb-supply-chain-basic.tanzu.vmware.com             Tanzu App Platform Out of The Box Supply Chain Basic\n  ootb-supply-chain-testing-scanning.tanzu.vmware.com  Tanzu App Platform Out of The Box Supply Chain with Testing and Scanning\n  ootb-supply-chain-testing.tanzu.vmware.com           Tanzu App Platform Out of The Box Supply Chain with Testing\n  ootb-templates.tanzu.vmware.com                      Tanzu App Platform Out of The Box Templates\n  policy.apps.tanzu.vmware.com                         Supply Chain Security Tools - Policy Controller\n  scanning.apps.tanzu.vmware.com                       Supply Chain Security Tools - Scan\n  service-bindings.labs.vmware.com                     Service Bindings for Kubernetes\n  services-toolkit.tanzu.vmware.com                    Services Toolkit\n  snyk.scanning.apps.tanzu.vmware.com                  Snyk for Supply Chain Security Tools - Scan\n  spring-boot-conventions.tanzu.vmware.com             Tanzu Spring Boot Conventions Server\n  spring-cloud-gateway.tanzu.vmware.com                Spring Cloud Gateway\n  sso.apps.tanzu.vmware.com                            AppSSO\n  tap-auth.tanzu.vmware.com                            Default roles for Tanzu Application Platform\n  tap-gui.tanzu.vmware.com                             Tanzu Application Platform GUI\n  tap-telemetry.tanzu.vmware.com                       Telemetry Collector for Tanzu Application Platform\n  tap.tanzu.vmware.com                                 Tanzu Application Platform\n  tekton.tanzu.vmware.com                              Tekton Pipelines\n  workshops.learningcenter.tanzu.vmware.com            Workshop Building Tutorial\n</code></pre>"},{"location":"tap/relocate-tap-images/","title":"Relocate TAP images","text":"<p>For air-gapped environments you need to relocate TAP images to an internal registry from Tanzu Network.</p> <p>First of all, you need an account on Tanzu Network and define a few variables for authenticating to it:</p> <pre><code>export IMGPKG_REGISTRY_HOSTNAME_0='registry.tanzu.vmware.com'\nexport IMGPKG_REGISTRY_USERNAME_0='&lt;TANZUNET-USERNAME&gt;'\nexport IMGPKG_REGISTRY_PASSWORD_0='&lt;TANZUNET-PASSWORD&gt;'\n</code></pre> <p>Define the <code>TAP_VERSION</code> variable, i.e.</p> <pre><code>TAP_VERSION='1.5.2'\n</code></pre> <p>You can list the available TAP versions either by logging into Tanzu Network or using pivnet:</p> <pre><code>pivnet rs --product-slug='tanzu-application-platform' --format yaml | yq 'map(select(.availability == \"All Users\")|.version)|sort_by(.)'\n</code></pre> <p>Download the TAP images bundle to a local tar file as</p> <pre><code>imgpkg copy \\\n-b registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:${TAP_VERSION} \\\n--to-tar tap-packages.tar \\\n--include-non-distributable-layers\n</code></pre> <p>Note</p> <p>If the current bastion host does not have access to the air-gapped environment, please move the downloaded tar file to a bastion that does and run the following commands on such host</p> <p>The file is around 9 GBs in size. Please make sure you have enough space on both hosts and file transfer between them is allowed.</p> <p>Define the following variables</p> <pre><code>export IMGPKG_REGISTRY_HOSTNAME_1='&lt;REGISTRY-FQDN&gt;'\nexport IMGPKG_REGISTRY_USERNAME_1='&lt;REGISTRY-USERNAME&gt;'\nexport IMGPKG_REGISTRY_PASSWORD_1='&lt;REGISTRY-PASSWORD&gt;'\n\nexport TAP_REPOSITORY=tap/tap-packages\nexport REGISTRY_CA_PATH=/path/to/registry-ca.pem\n</code></pre> <p>If using Harbor, make sure you have prepared a project to host TAP images, i.e. <code>tap</code>, and build the destination repository string accordingly.</p> <p>Create a Harbor project via API</p> <p>You can create a new Harbor project named <code>tap</code> with the following command, assuming the user you have set previously has admin rights:</p> <pre><code>curl -sSfL -X POST -u \"${IMGPKG_REGISTRY_USERNAME_1}:${IMGPKG_REGISTRY_PASSWORD_1}\" -d '{\"project_name\": \"tap\"}' -H \"Content-Type: application/json\" -H \"Accept: application/json\" https://${IMGPKG_REGISTRY_HOSTNAME_1}/api/v2.0/projects\n</code></pre> <p>Full Harbor API documentation is available at your own Harbor installation at <code>https://${IMGPKG_REGISTRY_HOSTNAME_1}/devcenter-api-2.0</code></p> <p>You can now store the images to the registry:</p> <pre><code>imgpkg copy \\\n--tar tap-packages.tar \\\n--to-repo ${IMGPKG_REGISTRY_HOSTNAME_1}/${TAP_REPOSITORY} \\\n--include-non-distributable-layers \\\n--registry-ca-cert-path ${REGISTRY_CA_PATH}\n</code></pre>"},{"location":"tap/relocate-tbs-dependencies/","title":"Relocate TBS dependencies","text":"<p>As soon as the TAP kapp repository has been created and the packages are available, in air-gapped environment it's mandatory to also relocate Tanzu Build Service images.</p> <p>Get the version of the buildservice package</p> <pre><code>tanzu package available list buildservice.tanzu.vmware.com --namespace tap-install\n</code></pre> <p>and store it to a variable, i.e.</p> <pre><code>TBS_VERSION=1.10.10\n</code></pre> <p>Do make sure that <code>IMGPKG_*</code> and the <code>REGISTRY_CA_PATH</code> variables are set in your shell, as explained in the relocate TAP images section.</p> <p>Download the TBS images bundle to a local tar file as</p> <pre><code>imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/full-tbs-deps-package-repo:${TBS_VERSION} --to-tar=tbs-full-deps.tar\n</code></pre> <p>Note</p> <p>If the current bastion host does not have access to the air-gapped environment, please move the downloaded tar file to a bastion that does and run the following commands on such host</p> <p>The file is around 12 GBs in size. Please make sure you have enough space on both hosts and file transfer between them is allowed.</p> <p>Define the following variables</p> <pre><code>export TBS_REPOSITORY=tap/tbs-full-deps\n</code></pre> <p>You can now store the images to the registry:</p> <pre><code>imgpkg copy --tar tbs-full-deps.tar \\\n--to-repo=${IMGPKG_REGISTRY_HOSTNAME_1}/${TBS_REPOSITORY} \\\n--registry-ca-cert-path ${REGISTRY_CA_PATH}\n</code></pre>"},{"location":"tkgm/","title":"Preface","text":"<p>The environment I'm testing TKG on is composed of a vSphere infrastructure (ESXi + vCenter), deployed along with a NSX-T platform to provide dynamic network segmentation and management.</p> <p>NSX-T is not strictly mandatory to follow along, it's just convenient for me for setting up the network segments, routers and policies to my own liking without the need for physical devices, as well as the ability to configure everything via API, that makes me able to provision an entire stack automatically within minutes.</p> <p>This section is about deploying and managing a Tanzu Kubernetes Grid 2.1 platform with a Standalone Management Cluster.</p> <p>What this guide is NOT meant for</p> <ul> <li>deploying vSphere with Tanzu</li> <li>giving comprehensive training around Kubernetes</li> <li>deep-diving into the tools used (i.e. Linux, bash, govc, Terraform, ...)</li> </ul> <p>However, some snippets are provided as a reference that you will need to adjust to your own environment, whilst some others are even ready to be used OOTB.</p> <p>Feedbacks are welcome</p> <p>If you think that something is not clear enough or totally wrong, please do create an issue and let me know.</p>"},{"location":"tkgm/bastion/","title":"Prepare bastion host","text":"<p>You will need a Linux bastion host equipped with a few tools to be able to fully deploy and operate TKGM. You can pick any distribution of choice, as long as you know how to do a basic configuration and deploy software on it. However, in this guide, I provide guidance on Ubuntu and RHEL-like (i.e. RHEL, CentOS, Oracle Linux) distributions only. It is also recommended not to use the <code>root</code> user, but instead create an unprivileged user (belonging to <code>wheel</code> group) and use it for day1 and day2 operations. In this guide I am using the <code>tanzu</code> user, that can be created as</p> <pre><code>useradd -m -G wheel tanzu\n</code></pre> <p>To allow the bastion host to relocate TKGM images, the minimum requirements for the bastion host are 2GB RAM, 2 vCPU and 45 GB free hard disk space, therefore it's better to provide at least ~10GB more for OS and packages installation.</p> <p>If you have fine-grained file systems, make sure you grant at least 20 GB free disk space for the <code>/tmp</code> directory, for storing temporary files during images relocation.</p>"},{"location":"tkgm/bastion/#proxy-configuration-optional","title":"Proxy configuration (optional)","text":"<p>NSX-T can be used to create segregated networks with proper policies to block egress traffic, and simulate an air-gapped environment for the TKGM cluster</p> <p>The bastion host is used to fetch the required images and copy them to the TKGM isolated network, therefore it somehow needs to access the Internet, i.e. via a proxy.</p> <p>For simulating a proxy we can use another host running a basic Squid installation. On Ubuntu, it can be installed as</p> <pre><code>apt install -y squid\n</code></pre> <p>By default, Squid denies all outgoing connections if not explicitly authorised via a <code>http_access</code> directive, like</p> <pre><code>acl external_allowed dstdomain \"/etc/squid/conf.d/allowed-domains.txt\"\nhttp_access allow external_allowed\n</code></pre>"},{"location":"tkgm/bastion/#software-installation","title":"Software installation","text":"<p>First of all, you need to identify a directory where you want to store those binaries. In the following guide, we'll be using the <code>LOCALBIN</code> variable you can set to whatever directory you are comfortable with, but make sure you also add it to your user's <code>PATH</code> environment variable. Also, if writing to it requires superuser privileges, do not forget to prepend every write command with <code>sudo</code>.</p> <pre><code>LOCALBIN=\"$HOME/.local/bin\"\nmkdir -p $LOCALBIN\nexport PATH=$PATH:$LOCALBIN\n</code></pre>"},{"location":"tkgm/bastion/#os-tools","title":"OS tools","text":"Oracle Linux <pre><code>sudo dnf install -y bash-completion bind-utils net-tools traceroute\n</code></pre>"},{"location":"tkgm/bastion/#git","title":"<code>git</code>","text":"<p>Most distributions come with <code>git</code> pre-installed, but in case yours doesn't you can leverage your package manager to get it:</p> Oracle LinuxUbuntu <pre><code>sudo dnf install -y git-all\n</code></pre> <pre><code>sudo apt install -y git\n</code></pre>"},{"location":"tkgm/bastion/#kubectl","title":"<code>kubectl</code>","text":"<p>The latest <code>kubectl</code> release can be installed as</p> <pre><code>curl -sSfLO \"https://dl.k8s.io/release/$(curl -sSfL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ninstall kubectl ${LOCALBIN}/kubectl\n</code></pre>"},{"location":"tkgm/bastion/#kubectx-and-kubens","title":"<code>kubectx</code> and <code>kubens</code>","text":"<p>Helper scripts to easily switch kubeconfig context and set default namespace. You may want to get the scripts from an actual release and not from the <code>main</code> branch.</p> <pre><code>LATEST=\"v0.9.4\"\nsudo git clone --depth 1 --branch $LATEST -c advice.detachedHead=false https://github.com/ahmetb/kubectx /opt/kubectx\nsudo ln -sf /opt/kubectx/kubectx ${LOCALBIN}/kubectx\nsudo ln -sf /opt/kubectx/kubens ${LOCALBIN}/kubens\n</code></pre>"},{"location":"tkgm/bastion/#kubectl-tree","title":"<code>kubectl-tree</code>","text":"<p>Show resources hierarchy</p> <pre><code>curl -sSfL https://github.com/ahmetb/kubectl-tree/releases/download/v0.4.3/kubectl-tree_v0.4.3_linux_amd64.tar.gz | tar xzf - kubectl-tree\nsudo install kubectl-tree ${LOCALBIN}\n</code></pre>"},{"location":"tkgm/bastion/#helm","title":"helm","text":"<pre><code>curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | HELM_INSTALL_DIR=~/.local/bin bash\n</code></pre>"},{"location":"tkgm/bastion/#fzf","title":"<code>fzf</code>","text":"<p><code>fzf</code> is a general-purpose command-line fuzzy finder.</p> <pre><code>curl -sSfL https://github.com/junegunn/fzf/releases/download/0.38.0/fzf-0.38.0-linux_amd64.tar.gz | tar xzf -\ninstall fzf ${LOCALBIN}/fzf\n</code></pre>"},{"location":"tkgm/bastion/#k9s","title":"<code>k9s</code>","text":"<pre><code>curl -sSfL https://github.com/derailed/k9s/releases/download/v0.27.3/k9s_Linux_amd64.tar.gz | tar xzf - k9s\ninstall k9s ${LOCALBIN}/k9s\n</code></pre>"},{"location":"tkgm/bastion/#stern","title":"<code>stern</code>","text":"<p>Stern is a CLI tool to tail logs from multiple pods and multiple containers at the same time.</p> <pre><code>curl -sSfL https://github.com/stern/stern/releases/download/v1.24.0/stern_1.24.0_linux_amd64.tar.gz | tar xzf - stern\nsudo install stern ${LOCALBIN}\n</code></pre>"},{"location":"tkgm/bastion/#yq","title":"<code>yq</code>","text":"<pre><code>curl -sSfL -o yq https://github.com/mikefarah/yq/releases/download/v4.30.8/yq_linux_amd64\ninstall yq ${LOCALBIN}/yq\n</code></pre>"},{"location":"tkgm/bastion/#jq","title":"<code>jq</code>","text":"Oracle LinuxUbuntu <pre><code>sudo dnf install -y jq\n</code></pre> <pre><code>sudo apt install -y jq\n</code></pre>"},{"location":"tkgm/bastion/#docker","title":"docker","text":"Oracle Linux <pre><code>sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nsudo dnf update -y\nsudo dnf install -y docker-ce docker-ce-cli containerd.io\n</code></pre> <p>Start and enable the <code>docker</code> service:</p> <pre><code>sudo systemctl start docker\nsudo systemctl enable docker\n</code></pre> <p>Finally, add the user to the <code>docker</code> group:</p> <pre><code>sudo gpasswd -a tanzu docker\n</code></pre>"},{"location":"tkgm/bastion/#vcc","title":"<code>vcc</code>","text":"<p><code>vcc</code> is a useful CLI tool for downloading packages from customerconnect.vmware.com. Get the latest release from https://github.com/vmware-labs/vmware-customer-connect-cli/releases, i.e.:</p> <pre><code>curl -sSfL -o vcc https://github.com/vmware-labs/vmware-customer-connect-cli/releases/download/v1.1.2/vcc-linux-v1.1.2\ninstall vcc ${LOCALBIN}/vcc\n</code></pre> <p>Some of the following steps require you to download software from Customer Connect. It is therefore necessary to set authentication credentials for <code>vcc</code> as environment variables <code>VCC_USER</code> and <code>VCC_PASS</code>, i.e.:</p> <pre><code>export VCC_USER=\"my-email-address@my-domain.com\"\nexport VCC_PASS='secret-P4ss'\n</code></pre> <p>Just make sure that</p> <ul> <li>you surround values with single-quotes and escape single-quotes characters in them.</li> <li>multi-factor authentication is disabled in your customerconnect profile, otherwise the CLI tool won't work (yet)</li> </ul>"},{"location":"tkgm/bastion/#govc","title":"<code>govc</code>","text":"<p><code>govc</code> allows you to interact with the vSphere vCenter appliance via CLI, thus being able to script your way to the configuration of your environment. Select the proper version/platform from https://github.com/vmware/govmomi/releases and install it, i.e.:</p> <pre><code>curl -sSfL https://github.com/vmware/govmomi/releases/download/v0.30.2/govc_Linux_x86_64.tar.gz | tar xzf - govc\ninstall govc ${LOCALBIN}/govc\n</code></pre>"},{"location":"tkgm/bastion/#carvel-suite","title":"Carvel suite","text":"<p>The Carvel suite is a set of useful tools for dealing with YAML files and OCI images.</p> <pre><code>curl -L https://carvel.dev/install.sh | K14SIO_INSTALL_BIN_DIR=${LOCALBIN} bash\n</code></pre>"},{"location":"tkgm/bastion/#tanzu-cli","title":"<code>tanzu</code> CLI","text":"<p>The <code>tanzu</code> CLI is available within the TKG product. Using <code>vcc</code> you can browse the products/subproducts/versions/files hierarchy and download exactly what you need. Make sure you choose the correct file for your platform (i.e. <code>linux-amd64</code>).</p> <p>For example,</p> <pre><code># list all the available products\nvcc get products\n\n# list all the available subproducts belonging to the vmware_tanzu_kubernetes_grid product\nvcc get subproducts -p vmware_tanzu_kubernetes_grid\n\n# list all the versions for the subproduct tkg\nvcc get versions -p vmware_tanzu_kubernetes_grid -s tkg\n\n# list all the files available for version 2.1.0\n# at this point vcc does the actual login\nvcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.0\n\n# download the tanzu cli\nvcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.0 -f 'tanzu-cli-bundle-linux-amd64.tar.gz' --accepteula\n</code></pre> <p>The downloaded files are available in the <code>~/vcc-downloads</code> directory.</p> <p>Now you can install the <code>tanzu</code> CLI:</p> <pre><code>tar xvzf vcc-downloads/tanzu-cli-bundle-linux-amd64.tar.gz\ninstall cli/core/v0.28.0/tanzu-core-linux_amd64 ${LOCALBIN}/tanzu\n</code></pre> <p>and the bundled plugins:</p> <pre><code>tar xzf tanzu-framework-plugins-standalone-linux-amd64.tar.gz -C cli\ntanzu plugin install all --local cli/standalone-plugins\n\ntar xzf tanzu-framework-plugins-context-linux-amd64.tar.gz -C cli\ntanzu plugin install all --local cli/context-plugins\n</code></pre> <p>Now you can get rid of the <code>cli</code> directory:</p> <pre><code>rm -rf cli\n</code></pre>"},{"location":"tkgm/identity-management-azuread/","title":"Azure AD","text":"<p>Azure AD (or Microsoft Entra in the near future) is an OIDC-compliant identity provider from Azure. It is available to all Azure subscriptions, even to the free one, although with some limitations.</p>"},{"location":"tkgm/identity-management-azuread/#register-a-new-application","title":"Register a new application","text":"<p>Sign in to the Azure Active Directory service in the Azure portal, and create a new registration following the \"App registrations\" link (the redirect URL can be set later).</p> <p>Each app will be associated to a name and a few IDs, as shown in the following image:</p> <p></p> <p>The \"Certificates &amp; secrets\" section allows for client secrets creation, used for authenticating Pinniped to Azure. As secrets require an expiration date, it is important to set up a process (either manual or automated) to rotate such secrets and prevent the app from becoming unusable.</p>"},{"location":"tkgm/identity-management-azuread/#token-configuration","title":"Token configuration","text":"<p>Token configuration is then required to include group memberships in the token itself, and thus be able to set up proper RBAC based on groups rather than users. Azure official docs explains how to properly add the groups claim, so does it list the limits in the number of groups that can be passed along into the token.</p> <p>Such limits can be easily reached in an enterprise environment, that's why either group filtering or app roles adoption can be applied to cope with them. Microsoft recommends to use App roles for new apps and when nested groups are not required. Users and groups can be then assigned to such roles, which are eventually passed to the token, via the <code>roles</code> claim, and used for RBAC in the app. Application roles are tipically fewer than the groups a user may be member of, and this guarantees that the token's constraints are honoured at all times.</p> <p>It is also possible to still include groups into the token but emit them as roles, and make the application request the <code>roles</code> claim.</p> <p></p> <p>One thing to remember is that for AzureAD-only groups, the Group ID will be included in the token and not its name, regardless of the choice made in the previous image. In fact, such groups have no concept of DNSDomain or sAMAccountName, and therefore the only unique property that can be used is the Group ID.</p> <p>You also need to add some optional claims, at least the <code>preferred_username</code> claim, that is going to be used as the user's identifier.</p> <p>You might also need to configure some API permissions, depending on your Azure AD setup. Please refer to your Azure admin for guidance.</p>"},{"location":"tkgm/identity-management-azuread/#define-variables","title":"Define variables","text":"<p>You can now define some variables that will be used for configuring Pinniped, replacing the <code>AZURE_*</code> placeholders with the values specific to your app:</p> <pre><code>IDP_NAME=\"azure\"\nOIDC_ISSUER_URL=\"https://login.microsoftonline.com/AZURE_TENANT_ID/\" # the trailing slash is mandatory\nOIDC_CLIENT_ID=\"AZURE_CLIENT_ID\"\nOIDC_CLIENT_SECRET=\"AZURE_CLIENT_SECRET\"\nOIDC_GROUPS_CLAIM=\"groups\"\nOIDC_USERNAME_CLAIM=\"preferred_username\"\nOIDC_SCOPES=\"email,profile,openid\"\n</code></pre> <p>Important</p> <ul> <li>The <code>AZURE_CLIENT_SECRET</code> placeholder refers to the actual secret value, not the ID.</li> <li>When using app roles or emitting groups as roles, the <code>OIDC_GROUPS_CLAIM</code> property in the above snippet must be set to <code>roles</code>.</li> </ul>"},{"location":"tkgm/identity-management-okta/","title":"Okta","text":"<p>You can sign up to an Okta developer account to get started quickly with an OIDC-compliant identity provider, and run some authentication and authorisation tests from your TKG platform.</p> <p>The simplest use case is to have users authenticate against Okta, retrieve the groups they belong to and assign Kubernetes permissions based on groups. RBAC is still configured on the Kubernetes end, whilst the authentication part is completely delegated to the IdP.</p>"},{"location":"tkgm/identity-management-okta/#create-a-new-application","title":"Create a new application","text":"<p>Log into your Okta developer account and go to <code>Applications</code> -&gt; <code>Applications</code> -&gt; <code>Create a new app integration</code>, select <code>OIDC - OpenID Connect</code> as sign-in method and <code>Web Application</code> as application type.</p> <p>Pick a name for the app (i.e. <code>tkgm</code>), enable the refresh token amd save; you will have to set the sign-in redirect URI when pinniped is up and running. Save the Client ID and the Client Secret, you will need them for configuring pinniped.</p>"},{"location":"tkgm/identity-management-okta/#create-users-and-groups","title":"Create users and groups","text":"<p>Now you may want to configure a few groups in <code>Directory</code> -&gt; <code>Groups</code> and assign people to them, as well as the application you just created, so that group members will be able to authenticate to your cluster. Initially you have just one user, whose username is the email address you registered to Okta with, but you can also create more if you wish, if you want to simulate a multi-user multi-group scenario.</p>"},{"location":"tkgm/identity-management-okta/#configure-authorization-server","title":"Configure authorization server","text":"<p>You need to ensure that group memberships are passed along in the JWT token when a user authenticates, and you must instruct the authorization server to do so.</p> <p>Go to <code>Security</code> -&gt; <code>API</code> and hit the pencil next to the <code>default</code> authorization server to modify it. Go to <code>Claims</code> and add a new claim: the name must be <code>groups</code>, included in <code>ID Token</code> type <code>Always</code>, and you may want to pass all the groups, with no filtering whatsoever. So the value type must be <code>Groups</code> and the filter <code>Matches regex</code> <code>.*</code>, and it must be included in <code>Any scope</code>.</p> <p>You can test the settings in the <code>Token Preview</code> tab.</p> <p>Further details are available in the official Okta docs.</p>"},{"location":"tkgm/identity-management-okta/#define-variables","title":"Define variables","text":"<p>You can now define some variables that will be used for configuring Pinniped, replacing the <code>OKTA_*</code> placeholders with the values specific to your app:</p> <pre><code>IDP_NAME=\"okta\"\nOIDC_ISSUER_URL=\"https://OKTA_DOMAIN\"\nOIDC_CLIENT_ID=\"OKTA_CLIENT_ID\"\nOIDC_CLIENT_SECRET=\"OKTA_CLIENT_SECRET\"\nOIDC_GROUPS_CLAIM=\"groups\"\nOIDC_USERNAME_CLAIM=\"email\"\nOIDC_SCOPES=\"email,profile,openid,groups\"\n</code></pre>"},{"location":"tkgm/identity-management/","title":"Identity management","text":"<p>Info</p> <p>The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-iam-index.html.</p> <p>This guide focuses on OIDC authentication and, as such, leverages Okta developer platform as an OIDC-compliant IdP.</p> <p>For the moment, it takes a ClickOps approach for testing things quickly, but in the future it is planned to switch to a more scripted and IaC-oriented methodology. An Okta Terraform provider is available, indeed, so the plan is to eventually configure everything with it, following this blog post.</p> <p>Pinniped is the component used for authenticating against LDAP or OIDC endpoints, and comes as a package in TKG. It is silently installed at deploy time, but can be configured either during management cluster deployment or afterwards.</p>"},{"location":"tkgm/identity-management/#configure-identity-provider","title":"Configure identity provider","text":"<p>Guides for configuring Okta and Azure AD OIDC endpoints are provided as a reference, and the configuration might be adapted seamlessly to other providers, although this is not guaranteed.</p> <p>You can then follow either guide to get started quickly with an OIDC-compliant identity provider, and run some authentication and authorisation tests from your TKG platform. The simplest use case is to have users authenticate against the IdP, retrieve the groups they're member of, and assign Kubernetes permissions based on groups. RBAC is still configured on the Kubernetes end, whilst the authentication part is completely delegated to the IdP.</p>"},{"location":"tkgm/identity-management/#configure-pinniped-during-management-cluster-deployment","title":"Configure pinniped during management cluster deployment","text":"<p>There are a few variables you need to set during management cluster installation, either via the GUI or the flat configuration file, as described in the section about TKG management cluster.</p> <p>Such variables, as described also in the VMware official docs, are</p> <pre><code>IDENTITY_MANAGEMENT_TYPE: oidc\nOIDC_IDENTITY_PROVIDER_CLIENT_ID: \"$OIDC_CLIENT_ID\"\nOIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \"$OIDC_CLIENT_SECRET\"\nOIDC_IDENTITY_PROVIDER_GROUPS_CLAIM: \"$OIDC_GROUPS_CLAIM\"\nOIDC_IDENTITY_PROVIDER_ISSUER_URL: \"$OIDC_ISSUER_URL\"\nOIDC_IDENTITY_PROVIDER_NAME: \"$IDP_NAME\"\nOIDC_IDENTITY_PROVIDER_SCOPES: \"$OIDC_SCOPES\"\nOIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \"$OIDC_USERNAME_CLAIM\"\n</code></pre> <p>They must be set in the flat configuration file and the deployment process will take care of the rest, generating TLS certificates too. For publicly trusted certificates (or at least within the enterprise, if a private CA is available) there will be a dedicated section.</p> <p>Note</p> <p>Those variables need to be set only in the management cluster; in fact, all workload clusters will inherit the same configuration from their own management cluster.</p>"},{"location":"tkgm/identity-management/#configure-pinniped-after-management-cluster-deployment","title":"Configure pinniped after management cluster deployment","text":"<p>OIDC authentication can be activated on existing management clusters, too. In fact, whether or not OIDC configs are passed to the management cluster during deployment, a <code>PackageInstall</code> resource for pinniped is created, but its reconciliation fails until a specific secret, containing the package's configuration values, is created in the <code>tkg-system</code> namespace. <code>kapp-controller</code> is on the lookout for such a secret and completes the package installation as soon as it is available.</p> <p>To get the secret manifest generated, you need to mock a flat configuration file and fake a TKG cluster creation, in order to get back the secret's YAML definition.</p> <p>Make a copy of your previous management cluster flat file and add the variables defined in the previous paragraph. Set a fake value for the <code>VSPHERE_CONTROL_PLANE_ENDPOINT</code> variable, to an unused IP, like</p> <pre><code>VSPHERE_CONTROL_PLANE_ENDPOINT: 1.1.1.1\n</code></pre> <p>otherwise the Tanzu CLI would fail during pre-flight checks, because the original endpoint is already taken by the existing cluster.</p> <p>Then set a few environment variables:</p> <pre><code>export IDENTITY_MANAGEMENT_TYPE=oidc\nexport _TKG_CLUSTER_FORCE_ROLE=\"management\"\nexport FILTER_BY_ADDON_TYPE=\"authentication/pinniped\"\n</code></pre> <p>Then generate the secret's YAML manifest:</p> <pre><code>tanzu cluster create &lt;CLUSTER-NAME&gt; --dry-run --file /path/to/the/yaml/config/file &gt; pinniped-secret.yaml\n</code></pre> <p>Make sure you pick the actual cluster name, as shown in</p> <pre><code>kubectl get clusters.cluster.x-k8s.io -n tkg-system\n</code></pre> <p>Do take a look at the generated <code>pinniped-secret.yaml</code> file, and change it if your cluster is behind a proxy. In fact, proxy configurations do not get passed along to the pinniped values, and so you have to set the values for:</p> <ul> <li><code>http_proxy</code></li> <li><code>https_proxy</code></li> <li><code>no_proxy</code></li> </ul> <p>The <code>no_proxy</code> variable is a comma-separated list of IP or domains patterns (this is worth a read). Remember to add to it also the CIDR range of your services network (i.e. <code>100.64.0.0/13</code>).</p> <p>Then you must apply the YAML manifest to the cluster</p> <pre><code>kubectl apply -f pinniped-secret.yaml\n</code></pre> <p>You can monitor the status of the configuration looking at the resources in the <code>pinniped-supervisor</code> namespace. In the end, you should have something like</p> <pre><code>\u276f kubectl -n pinniped-supervisor get all\n\nNAME                                                   READY   STATUS      RESTARTS   AGE\npod/pinniped-post-deploy-controller-64c59657c5-hsd5w   1/1     Running     0          109m\npod/pinniped-post-deploy-job-scstf                     0/1     Completed   0          59m\npod/pinniped-supervisor-77d67dc647-f7bpd               1/1     Running     0          59m\npod/pinniped-supervisor-77d67dc647-t8fqq               1/1     Running     0          59m\n\nNAME                          TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nservice/pinniped-supervisor   NodePort   100.69.192.203   &lt;none&gt;        443:31234/TCP   109m\n\nNAME                                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/pinniped-post-deploy-controller   1/1     1            1           109m\ndeployment.apps/pinniped-supervisor               2/2     2            2           109m\n\nNAME                                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/pinniped-post-deploy-controller-64c59657c5   1         1         1       109m\nreplicaset.apps/pinniped-supervisor-77d67dc647               2         2         2       109m\n\nNAME                                 COMPLETIONS   DURATION   AGE\njob.batch/pinniped-post-deploy-job   1/1           8s         59m\n</code></pre> <p>It is worth to check the <code>pinniped-concierge</code> namespace as well:</p> <pre><code>\u276f kubectl -n pinniped-concierge get all\n\nNAME                                                     READY   STATUS    RESTARTS   AGE\npod/pinniped-concierge-5f85876ff6-fpq9t                  1/1     Running   0          111m\npod/pinniped-concierge-5f85876ff6-n497d                  1/1     Running   0          111m\npod/pinniped-concierge-kube-cert-agent-f6fbff54f-7jtfp   1/1     Running   0          110m\n\nNAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/pinniped-concierge-api     ClusterIP   100.71.104.69   &lt;none&gt;        443/TCP   111m\nservice/pinniped-concierge-proxy   ClusterIP   100.71.213.71   &lt;none&gt;        443/TCP   111m\n\nNAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/pinniped-concierge                   2/2     2            2           111m\ndeployment.apps/pinniped-concierge-kube-cert-agent   1/1     1            1           110m\n\nNAME                                                           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/pinniped-concierge-5f85876ff6                  2         2         2       111m\nreplicaset.apps/pinniped-concierge-kube-cert-agent-f6fbff54f   1         1         1       110m\n</code></pre> <p>Info</p> <p>Using Kube-VIP, the only way to expose pinniped is via a NodePort service, thus do remember to set the appropriate rules in your network firewalls to allow the traffic to port 31234/tcp.</p>"},{"location":"tkgm/identity-management/#provide-own-tls-certificates","title":"Provide own TLS certificates","text":"<p>https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-iam-custom-pinniped-certificates.html</p>"},{"location":"tkgm/identity-management/#configure-redirect-uri","title":"Configure redirect URI","text":"<p>Now that you have pinniped configured and running, you can go back to your IdP configuration portal and set the redirect URI for your application. Edit the <code>General Settings</code> section of your application and set the sign-in redirect URI to <code>https://&lt;YOUR-MGMT-CLUSTER-ENDPOINT&gt;:31234/callback</code>. Use the very same endpoint you set in your certificate, if you happened to change it.</p>"},{"location":"tkgm/identity-management/#get-the-kubeconfig-file","title":"Get the kubeconfig file","text":"<p>Use the Tanzu CLI to retrieve the kubeconfig file to authenticate via OIDC:</p> <pre><code>tanzu mc kubeconfig get --export-file /tmp/tkgm.oidc.kubeconfig\n</code></pre> <p>Open the file and look at the <code>users</code> object; the <code>tanzu pinniped-auth login</code> command is used to authenticate the user, with a few configuration options:</p> <ul> <li>make sure the <code>--issuer</code> flag points to the same endpoint as the redirect URI (without the trailing <code>/callback</code>);</li> <li>add the item <code>--skip-browser</code> to the end of the <code>args</code> list, to prevent the Tanzu CLI from trying to run a browser to let the user authenticate (on SSH-based jumphosts you do not have nor you do want to have a GUI).</li> </ul> <p>Run a <code>kubectl get</code> command with the new kubeconfig file to get prompted for authentication</p> <pre><code>\u276f kubectl get pods --kubeconfig ~/t1xl/t1xl.oidc.kubeconfig\nLog in by visiting this link:\n\n    https://my.cluster.endpoint:31234/oauth2/authorize?access_type=offline&amp;client_id=pinniped-cli&amp;code_challenge=lGjmpPEfyN0544UEOr2QYWawsWBzKOorlQG-Ws73FHs&amp;code_challenge_method=S256&amp;nonce=48cd4fc34b4e789ee8e6e590ae54aab0&amp;redirect_uri=http%3A%2F%2F127.0.0.1%3A45359%2Fcallback&amp;response_mode=form_post&amp;response_type=code&amp;scope=offline_access+openid+pinniped%3Arequest-audience&amp;state=93b02cc91ad38b8e905844abcf0c6144\n\n    Optionally, paste your authorization code:\n</code></pre> <p>Paste the URL in your browser, authenticate to Okta and you get a code back that you have to paste into the console: you are now authenticated. You can see your token details (including the groups that have been passed along) at <code>~/.config/tanzu/pinniped/sessions.yaml</code>.</p> <p>Your request is going to fail unless you have already configured RBAC on your cluster.</p>"},{"location":"tkgm/identity-management/#configure-rbac","title":"Configure RBAC","text":"<p>If you have correctly configured users and groups and the authorization server, you should now find a list of groups in the <code>~/.config/tanzu/pinniped/sessions.yaml</code> file for your TKG ID token.</p> <p>The authorization part still lies on the Kubernetes part, leveraging <code>Role</code>s and <code>ClusterRole</code>s for defining permissions, and <code>RoleBinding</code>s and <code>ClusterRoleBinding</code>s for granting them to either users or groups.</p> <p>TKG comes with a few pre-defined <code>ClusterRole</code>s</p> <ul> <li>cluster-admin: Full access to the cluster. When used in a RoleBinding, this role gives full access to any resource in the namespace specified in the binding.</li> <li>admin: Admin access to most resources in a namespace. Can create and modify roles and role bindings within the namespace.</li> <li>edit: Read-write access to most objects in a namespace, such as deployments, services, and pods. Cannot view, create, or modify roles and role bindings.</li> <li>view: Read-only access to most objects in a namespace. Cannot view, create, or modify roles and role bindings</li> </ul> <p>More info at VMware docs.</p> <p>As an example, a simple <code>ClusterRoleBinding</code> that grants a group <code>view</code> permissions on all namespaces can be defined as:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: dev-view\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: view\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\nkind: Group\nname: dev\n</code></pre>"},{"location":"tkgm/ingress/","title":"Ingress controller","text":""},{"location":"tkgm/ingress/#kube-vip-load-balancer","title":"Kube-VIP Load Balancer","text":"<p>This topic describes using Kube-VIP as a load balancer for workloads hosted on Tanzu Kubernetes Grid (TKG) workload clusters.</p> <p>Technical Preview</p> <p>This feature is still a technical preview, yet very helpful to quickly create a load balancer to be used in a lab environment, and that will allegedly be fully supported on TKG future releases.</p> <p>To learn how to set it up see official VMware docs.</p>"},{"location":"tkgm/ingress/#nginx-ingress-controller","title":"NGINX ingress controller","text":"<p>You can install the NGINX ingress controller in a number of ways, detailed in the NGINX docs website.</p>"},{"location":"tkgm/ingress/#installation-via-helm-chart","title":"Installation via Helm chart","text":"<p>You can pull the NGINX ingress sources from GitHub</p> <pre><code>git clone https://github.com/nginxinc/kubernetes-ingress.git --branch v3.0.2\ncd kubernetes-ingress/deployments/helm-chart\n</code></pre> <p>or install a Helm repo and pull the chart</p> <pre><code>helm repo add nginx-stable https://helm.nginx.com/stable\nhelm repo update\nhelm pull nginx-stable/nginx-ingress --destination . --untar\ncd nginx-ingress\n</code></pre> <p>Either way, if your TKG instance lives in an air-gapped environment, you have to download the helm chart from an Internet-connected host and then copy it over to the bastion host inside the environment.</p> <p>You also need to relocate the <code>nginx/nginx-ingress</code> image to the internal registry. Don't forget to provide DockerHub authentication credentials (create a read-only temporary Personal Access Token) to avoid getting throttled.</p> <pre><code>export IMGPKG_REGISTRY_HOSTNAME_0='&lt;REGISTRY-FQDN&gt;'\nexport IMGPKG_REGISTRY_USERNAME_0='&lt;REGISTRY-USERNAME&gt;'\nexport IMGPKG_REGISTRY_PASSWORD_0='&lt;REGISTRY-PASSWORD&gt;'\n\nexport IMGPKG_REGISTRY_HOSTNAME_1='index.docker.io'\nexport IMGPKG_REGISTRY_USERNAME_1='&lt;DOCKERHUB-USERNAME&gt;'\nexport IMGPKG_REGISTRY_PASSWORD_1='&lt;DOCKERHUB-PAT&gt;'\n\nimgpkg copy -i nginx/nginx-ingress:3.0.2 --to-repo registry.my.domain/apps/nginx-ingress --registry-ca-cert-path harbor-ca.pem\n</code></pre> <p>Provide a values file for the helm installation to specify your custom settings, i.e.:</p> <ul> <li>the URL to the relocated image;</li> <li>the <code>service.externalTrafficPolicy</code> set to <code>Cluster</code>;</li> <li>the service type and IP address, to make sure it is static and won't change.</li> </ul> <pre><code>controller:\nimage:\nrepository: registry.my.domain/apps/nginx-ingress\ntag: 3.0.2\n\nservice:\ntype: LoadBalancer\nloadBalancerIP: 172.31.9.21\nexternalTrafficPolicy: Cluster\n</code></pre> Brief explanation about the <code>externalTrafficPolicy</code> <p>The helm chart's default value is <code>Local</code>, which allows to preserve the client IP address but also keeps the connection local to the node, with no chance for it to be routed to other nodes in the cluster.</p> <p>However, with the default setting, using a load balancer such as Kube-VIP that allocates IP addresses to the control plane nodes (thus assigned to the services of type <code>LoadBalancer</code>), makes it impossible for external connections to reach the actual application pods, as they do not and never will run on any control plane nodes.</p> <p>See also Kubernetes docs.</p> <p>You can also customise it further if you want; all the available parameters can be listed as</p> <pre><code>helm show values .\n</code></pre> <p>Finally, you can start the installation:</p> <pre><code>helm install nginx-ingress . --values /path/to/the/values/file.yaml\n</code></pre>"},{"location":"tkgm/ingress/#deploy-test-http-application","title":"Deploy test HTTP application","text":"<p>For testing, you can deploy a NGINX web server and expose its default welcome page through the ingress controller, to check connectivity from both within the cluster and outside of it.</p> <p>First of all, you have to relocate the image (<code>nginx/nginx:stable-alpine</code>, in this example) to the internal registry like you did for the ingress controller image.</p> <p>Then, you can create a Deployment and its Service</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: app1\nname: app1\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: app1\ntemplate:\nmetadata:\nlabels:\napp: app1\nspec:\ncontainers:\n- image: registry.my.domain/apps/nginx:stable-alpine\nname: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp: app1\nname: app1\nspec:\nports:\n- port: 80\nprotocol: TCP\ntargetPort: 80\nselector:\napp: app1\n</code></pre> <p>and an Ingress resource accordingly</p> ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: app1\nannotations:\nnginx.org/rewrites: \"serviceName=app1 rewrite=/\"\nspec:\ningressClassName: nginx\nrules:\n- host: \"*.my.domain\"\nhttp:\npaths:\n- path: /app1\npathType: Prefix\nbackend:\nservice:\nname: app1\nport:\nnumber: 80\n</code></pre> <p>In this example, the Ingress resource uses a wildcard to listen on all hosts (that end up being associated to the ingress controller IP address), and allows you to define multiple paths that can route the traffic to multiple backend applications.</p> <p>You should now create a DNS record to associate <code>app1.my.domain</code> to the ingress controller service's IP address <code>172.31.9.21</code>, or you could just forge the <code>Host</code> header in the test HTTP request:</p> <pre><code>\u276f curl -H \"Host: app1.my.domain\" http://172.31.9.21/app1\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>This proves that the configuration is correct.</p>"},{"location":"tkgm/management-cluster/","title":"TKG management cluster","text":"<p>Info</p> <p>The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/tkg-deploy-mc-21/mgmt-index.html.</p>"},{"location":"tkgm/management-cluster/#import-node-os-images","title":"Import node OS images","text":"<p>TKGm can be run either on Ubuntu or Photon VMs, whose base OS images are available on Customer Connect.</p> <pre><code>\u276f vcc get versions -p vmware_tanzu_kubernetes_grid -s tkg\n...\nFILENAME                                                                       SIZE        BUILD NUMBER  DESCRIPTION\n...\nphoton-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova      1.02 GB     21158328      Photon v3 Kubernetes v1.24.9 OVA\nphoton-3-kube-v1.23.15+vmware.1-tkg.1-2d8c52b5e3dc2b7a03bf3a04815b9474.ova     1022.45 MB  21158328      Photon v3 Kubernetes v1.23.15 OVA\nphoton-3-kube-v1.22.17+vmware.1-tkg.1-a10ef8e088cc2c89418bca79a2fcc594.ova     1 GB        21158328      Photon v3 Kubernetes v1.22.17 OVA\nubuntu-2004-kube-v1.24.9+vmware.1-tkg.1-b030088fe71fea7ff1ecb87a4d425c93.ova   1.78 GB     21158328      Ubuntu 2004 Kubernetes v1.24.9 OVA\nubuntu-2004-kube-v1.23.15+vmware.1-tkg.1-66940e9f3a5eda8bbd51042b1aaea6e1.ova  1.74 GB     21158328      Ubuntu 2004 Kubernetes v1.23.15 OVA\nubuntu-2004-kube-v1.22.17+vmware.1-tkg.1-df08b304658a6cf17f5e74dc0ab7543c.ova  1.77 GB     21158328      Ubuntu 2004 Kubernetes v1.22.17 OVA\n...\n</code></pre> <p>You might want to select the latest version of a particular flavour, to benefit from the latest updates and getting closer to the most recent upstream version. In this case, we're going to pick the Photon image for Kubernetes v1.24.9</p> <pre><code>vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.0 -f 'photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova' --accepteula\n</code></pre> <p>and import it to our vSphere content library, to make sure it's available when needed.</p> <pre><code>govc library.import -n photon-1.24.9 ova $HOME/vcc-downloads/photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova\n</code></pre> <p>However, TKGm expects a VM template to be available, therefore we need to deploy a template from this content library item. We dump the specs into a JSON file</p> <pre><code>govc import.spec $HOME/vcc-downloads/photon-3-kube-v1.24.9+vmware.1-tkg.1-f5e94dab9dfbb9988aeb94f1ffdc6e5e.ova &gt; photon.json\n</code></pre> <p>and we edit it to make sure that</p> <ul> <li>it is marked as a template</li> <li>it does not get powered on</li> <li>the <code>DiskProvisioning</code> configuration is correct</li> </ul> <p>The result looks like this</p> <pre><code>{\n\"DiskProvisioning\": \"thin\",\n\"IPAllocationPolicy\": \"dhcpPolicy\",\n\"IPProtocol\": \"IPv4\",\n\"NetworkMapping\": [\n{\n\"Name\": \"nic0\",\n\"Network\": \"\"\n}\n],\n\"Annotation\": \"Cluster API vSphere image - VMware Photon OS 64-bit and Kubernetes v1.24.9+vmware.1 - https://github.com/kubernetes-sigs/cluster-api-provider-vsphere\",\n\"MarkAsTemplate\": true,\n\"PowerOn\": false,\n\"InjectOvfEnv\": false,\n\"WaitForIP\": false,\n\"Name\": null\n}\n</code></pre> <p>Finally, we can deploy the OVA into a folder dedicated to our cluster</p> <pre><code>govc folder.create /vc01/vm/tkgm\ngovc library.deploy -options photon.json -folder tkgm /ova/photon-1.24.9\n</code></pre>"},{"location":"tkgm/management-cluster/#prepare-management-cluster-configuration","title":"Prepare management cluster configuration","text":"<p>Tanzu Kubernetes Grid standalone cluster deployment can be run in unattended mode via the Tanzu CLI, feeding it with a YAML configuration file. Alternatively, a web UI is also available for guiding the user through all the required settings.</p> <p>There is actually a third option, which is preferrable if you are not familiar with or not entirely sure about all the different settings available: it consists of using the web UI for generating the initial YAML file, and then fine tuning it, if needed, before submitting it to the Tanzu CLI. At this point, the YAMNL descriptor can also be stored in some git repo and used as a template for future installations.</p> <p>You use the Tanzu CLI to start the web UI</p> <pre><code>tanzu mc create --ui\n</code></pre> <p>which by default will listen on <code>127.0.0.1:8080</code>. If you run it from the bastion host, you can either use the <code>--bind</code> flag or you need to create a ssh tunnel to the bastion like</p> <pre><code>ssh -l &lt;BASTION-USER&gt; -L 8080:localhost:8080 &lt;BASTION-HOST-IP-OR-FQDN&gt;\n</code></pre> <p>and then direct your browser to http://localhost:8080.</p> <p>Now you can go through step-by-step process, providing the required information (i.e. vCenter endpoint and credentials, cluster details, ...). In the end, you review your configuration and are able to go back to change some bits; when everything looks good, you can go ahead and deploy the management cluster directly or export the generated YAML descriptor.</p> <p>Do make sure you export the file and go through it to get yourself comfortable with all the settings. Also, if you need to set up OIDC authentication, please take a look at the identity management document.</p>"},{"location":"tkgm/management-cluster/#prepare-the-tools","title":"Prepare the tools","text":"<p>If you are planning to run TKGm in an isolated environment, you should have already relocated all the images, and now you need to make the Tanzu CLI aware of your internal registry. You can do so by setting the following variables:</p> <ul> <li><code>TKG_CUSTOM_IMAGE_REPOSITORY</code> stores the base URL of the images, which is <code>&lt;REGISTRY&gt;/&lt;PROJECT&gt;</code></li> <li><code>TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE</code> stores the base64-encoded certificate of the CA that trusts the registry's certificate</li> </ul> <p>Warning</p> <p>The value of <code>TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE</code> MUST NOT contain newline characters.</p> <p>These values can be set either as environment variables or Tanzu CLI's variables.</p> Tanzu CLI variables (recommended)Environment variables <p>You add the variables to the Tanzu CLI configuration, which is stored in a file thus persisted across different shells. This is the preferred method when your bastion host is used to manage only one management cluster or registry.</p> <pre><code>tanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY \"&lt;REGISTRY-FQDN&gt;/tkg\"\ntanzu config set env.TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE \"$(base64 -w0 /path/to/harbor-ca.pem)\"\n</code></pre> <p>This is the preferred method if you have to deal with multiple environments with different configurations (i.e. different OCI registries), which is not this case.</p> <pre><code>export TKG_CUSTOM_IMAGE_REPOSITORY=\"&lt;REGISTRY-FQDN&gt;/tkg\"\nexport TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE=\"$(base64 -w0 &lt; /path/to/harbor-ca.pem)\"\n</code></pre> <p>Do make sure your Docker client trusts the registry as explained in the dedicated section.</p>"},{"location":"tkgm/management-cluster/#deploy-the-management-cluster","title":"Deploy the management cluster","text":"<p>It's finally time to build the management cluster.</p> <p>You need to add one more bit to your configuration file, otherwise the Tanzu CLI will prompt you for confirmation about deploying a management cluster on top of vSphere rather than an integrated vSphere with Tanzu (Tanzu Kubernetes Grid service).</p> <pre><code>DEPLOY_TKG_ON_VSPHERE7: true\n</code></pre> <p>Now you can create your management cluster as</p> <pre><code>tanzu mc create --file /path/to/the/yaml/config/file -v 6\n</code></pre>"},{"location":"tkgm/registry/","title":"Deployment and configuration","text":"<p>A very common scenario for customers adopting Tanzu Kubernetes Grid platform is running it within an isolated environment, be it either just Internet-restricted or even air-gapped. To be able to tackle such situations, it is necessary to fulfil a few tasks and meet some prerequisites.</p> <p>Tanzu Kubernetes Grid images, as well as applications', must be available from within the isolated environment, thus it's necessary to fetch and make them available in advance to the platform via an internal OCI registry. VMware supports CNCF's Harbor, and, since TKGM 2.1.0, provides a ready-to-use OVA template, downloadable from Customer Connect.</p>"},{"location":"tkgm/registry/#download-template","title":"Download template","text":"<p>After you've downloaded <code>vcc</code> and authenticated as described in the bastion host preparation guide, you can fetch the Harbor OVA:</p> <p>First of all, list the files available for tkg 2.1.0</p> <pre><code>vcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.0\n</code></pre> <p>and find the Harbor OVA file:</p> <pre><code>FILENAME                                                                       SIZE        BUILD NUMBER  DESCRIPTION\n...\nphoton-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova   1.27 GB                   Photon v4 Harbor v2.6.3 OVA\n...\n</code></pre> <p>Save the name to a variable, for DRY purposes:</p> <pre><code>HARBOR_OVA=\"photon-4-harbor-v2.6.3+vmware.1-9c5c48c408fac6cef43c4752780c4b048e42d562.ova\"\n</code></pre> <p>Then download the file:</p> <pre><code>vcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.0 -f ${HARBOR_OVA} --accepteula\n</code></pre> <p>The file will be stored by default at <code>$HOME/vcc-downloads/</code>.</p>"},{"location":"tkgm/registry/#install-appliance","title":"Install appliance","text":"<p>You can now deploy a VM out of the downloaded OVA using <code>govc</code>. Make sure it is installed and authentication has been properly configured, like:</p> <pre><code>export GOVC_URL=my.vcenter.fqdn\nexport GOVC_USERNAME=my.vcenter.username\nexport GOVC_PASSWORD=my.vcenter.password\nexport GOVC_INSECURE=true\n</code></pre> <p>The last line is needed if the vCenter certificate is not yet trusted by your bastion host.</p> <p>You need a configuration file for customising the template to your needs and produce an actual VM. Please export the spec to a file as</p> <pre><code>govc import.spec $HOME/vcc-downloads/${HARBOR_OVA} | jq &gt; harbor.json\n</code></pre> <p>and you get something like the following sample JSON snippet</p> harbor.json <pre><code>{\n\"DiskProvisioning\": \"flat\",\n\"IPAllocationPolicy\": \"dhcpPolicy\",\n\"IPProtocol\": \"IPv4\",\n\"PropertyMapping\": [{\n\"Key\": \"guestinfo.root_password\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.allow_root_ssh\",\n\"Value\": \"True\"\n},\n{\n\"Key\": \"guestinfo.harbor_hostname\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.harbor_admin_password\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.harbor_database_password\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.harbor_scanner_enable\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.harbor_selfsigned_cert\",\n\"Value\": \"True\"\n},\n{\n\"Key\": \"guestinfo.harbor_ca\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.harbor_server_cert\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.harbor_server_key\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.network_ip_address\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.network_netmask\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.network_gateway\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.network_dns_server\",\n\"Value\": \"\"\n},\n{\n\"Key\": \"guestinfo.network_dns_domain\",\n\"Value\": \"\"\n}\n],\n\"NetworkMapping\": [{\n\"Name\": \"nic0\",\n\"Network\": \"\"\n}],\n\"Annotation\": \"Harbor ova vSphere image - VMware Photon OS 64-bit and Harbor v2.6.3+vmware.1\",\n\"MarkAsTemplate\": false,\n\"PowerOn\": false,\n\"InjectOvfEnv\": false,\n\"WaitForIP\": false,\n\"Name\": null\n}\n</code></pre> <p>which you can tune to your liking by filling in the actual values for your environment (see following tips)</p> Tip <p>A few notes about properties in the <code>PropertyMapping</code> array:</p> <p>Passwords</p> <p>The following passwords have to be at least 8 and at most 128 characters long:</p> <ul> <li><code>guestinfo.root_password</code></li> <li><code>guestinfo.harbor_admin_password</code></li> </ul> <p>No other complexity requirements have to be met.</p> <p>DiskProvisioning</p> <p>Make sure you do specify the correct value for <code>DiskProvisioning</code> (i.e. <code>thin</code>), or you may hit an error like</p> <pre><code>govc: deploy error: Invalid value for DatastoreMappingParams.target_provisioning_type: flat.\n</code></pre> <p>Harbor hostname</p> <p>Set the <code>guestinfo.harbor_hostname</code> property to the fully-qualified domain name you want to use to reach the service.</p> <p>If the certificate gets auto-generated, this is the value that is used as Common Name (and SAN, too), otherwise, it must match with the certificate you provide.</p> <p>Harbor scanner</p> <p>Set the <code>guestinfo.harbor_scanner_enable</code> property to either <code>\"True\"</code> or <code>\"False\"</code> (as a string), to avoid the following error at power on:</p> <pre><code>govc: Property 'Enable Harbor Default Scanner' must be configured for the VM to power on.\n</code></pre> <p>Certificates and keys</p> <p>CA and server certificates and key must be supplied in plain PEM format, not base64-encoded. If no values are provided they will be auto-generated, thus they will be self-signed (not recommended for production use).</p> <p>PowerOn</p> <p>If set to <code>true</code>, the VM will be powered on as soon as it's created.</p> <p>WaitForIP</p> <p>If set to <code>true</code>, <code>govc</code> will not return until the VM acquires an IP address.</p> <p>Finally, you run the following command to deploy a new virtual machine (named <code>registry</code>):</p> <pre><code>govc import.ova -options harbor.json -name registry $HOME/vcc-downloads/${HARBOR_OVA}\n</code></pre>"},{"location":"tkgm/registry/#trust-your-registry","title":"Trust your registry","text":"<p>You need to make sure that the Docker client on the bastion host trusts the internal registry. In fact, when you will get to deploy Tanzu Kubernetes Grid, the Tanzu CLI will use the Docker client to run a local kind cluster, that will eventually deploy all the required components to the actual location (i.e. your vSphere cluster). The Docker client will attempt to pull the kind image from the internal registry, and will ultimately fail if the trust is not established.</p> <p>Docker verifies a registry certificate against system CAs and additional CA certificates at <code>/etc/docker/certs.d/&lt;REGISTRY-FQDN&gt;/ca.crt</code>. If the URL of the registry you want to interact with has a port you need to specify it as well. See Docker docs for more information.</p> <p>So you need to copy the CA certificate into the above location as</p> <pre><code>sudo mkdir /etc/docker/certs.d/&lt;REGISTRY-FQDN&gt;\nsudo cp /path/to/harbor-ca.pem /etc/docker/certs.d/&lt;REGISTRY-FQDN&gt;/ca.crt\nsudo systemctl reload docker\n</code></pre>"},{"location":"tkgm/registry/#configure-tkg-project","title":"Configure <code>tkg</code> project","text":"<p>Tanzu Kubernetes Grid images must be loaded into a dedicated project in Harbor, a hierarchical object that acts as a folder and allows to set specific configurations in terms of, e.g., permissions and quota.</p> <p>Navigating to the Harbor UI<sup>1</sup> as <code>admin</code> user, you can follow the <code>Projects</code> link, and create a new project hitting the <code>+ NEW PROJECT</code> button. The pop-up modal window allows you to set the name (we can name it <code>tkg</code>), the quota and whether or not it should be publicly accessible (no authentication required). More granular settings can be applied by editing the project after it has been created.</p> <p>Warning</p> <p>For TKG to successfully pull platform images, the <code>tkg</code> Harbor project must be public.</p> <p>If concerned about security implications, please note that</p> <ul> <li>images are publicly available anyways at <code>projects.registry.vmware.com/tkg</code>;</li> <li>public access means that unauthorised users (which includes the unauthenticated ones) are allowed to pull but not to push images, thus not allowed to inject malicious code.</li> </ul>"},{"location":"tkgm/registry/#configure-users","title":"Configure users","text":"<p>You now need to create a user with write permissions to the <code>tkg</code> project created previously. You can do so by navigating to the <code>Administration &gt; Users</code> menu in the Harbor UI<sup>1</sup>.</p> <p>Once the user has been created, it needs to be authorised to the <code>tkg</code> project, granting it at least the <code>developer</code> role: go back to <code>Projects</code>, click on the <code>tkg</code> project to edit it and add a user selecting the correct role. More info on roles at the official Harbor docs.</p> <ol> <li> <p>in the future, I might want to add a section to this guide for configuring Harbor programmatically, either by <code>curl</code>ing APIs, or leveraging tools like Terraform or Ansible.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"tkgm/relocate-images/","title":"Relocate TKG images","text":"<p>Tanzu Kubernetes Grid images are publicly available at <code>projects.registry.vmware.com/tkg</code>, for serving TKG installations.</p> <p>The image relocation process allows you to pull those images and push them to your own OCI registry, for easier consumption by the TKG management and workload clusters you deploy across your environments. This is a pre-requisite for air-gapped installations, but is always highly beneficial as it can dramatically speed up image fetching during deployments, saving time and Internet bandwidth.</p> <p>The Tanzu CLI comes with a handy plugin that allows you to easily relocate TKG images, as better described in TKG docs.</p> <p>First of all, you need to download all the required images to your local file system</p> <pre><code>mkdir -p tkgm-bundle &amp;&amp; cd tkgm-bundle\ntanzu isolated-cluster download-bundle \\\n--source-repo projects.registry.vmware.com/tkg \\\n--tkg-version v2.1.0\n</code></pre> <p>you copy the entire <code>tkgm-bundle</code> directory into your air-gapped environment<sup>1</sup>, and then you can upload the images to your registry.</p> <p>The Tanzu CLI uses <code>imgpkg</code> under the hood, so you manage authentication as described in <code>imgpkg</code> docs. For example, you can use environment variables, like:</p> <pre><code>export IMGPKG_REGISTRY_HOSTNAME_0='&lt;REGISTRY-FQDN&gt;'\nexport IMGPKG_REGISTRY_USERNAME_0='&lt;REGISTRY-USERNAME&gt;'\nexport IMGPKG_REGISTRY_PASSWORD_0='&lt;REGISTRY-PASSWORD&gt;'\nREGISTRY_CA_PATH=/path/to/registry-ca.pem\n</code></pre> <p>and then you can upload the images using the <code>tanzu isolated-cluster upload-bundle</code> command. The username and password have been set during Harbor preparation.</p> <p>You also need the registry's CA chain certificates, to verify the endpoint when pushing images. When using Harbor v2.x as a registry, you can download the certificates to a file via</p> <pre><code>curl -sSfLk -H 'accept: application/octet-stream' -o ${REGISTRY_CA_PATH} https://&lt;REGISTRY-FQDN&gt;/api/v2.0/systeminfo/getcert\n</code></pre> <p>but if you have a different registry you can also fetch the full certificate chain via openssl:</p> <pre><code>openssl s_client -connect &lt;REGISTRY-FQDN&gt;:443 -servername &lt;REGISTRY-FQDN&gt; -showcerts &lt;/dev/null 2&gt;/dev/null | sed -n '/-----BEGIN CERTIFICATE-----/, /-----END CERTIFICATE-----/p' &gt; ${REGISTRY_CA_PATH}\n</code></pre> <p>This command fetches the leaf certificate, too, and works with every SSL-enabled endpoint. Do make sure to change the port according to your registry if it's not 443.</p> <p>Then you must push the images</p> <pre><code>tanzu isolated-cluster upload-bundle \\\n--source-directory tkgm-bundle \\\n--destination-repo \"&lt;REGISTRY-FQDN&gt;/tkg\" \\\n--ca-certificate ${REGISTRY_CA_PATH}\n</code></pre> <p>Note</p> <p>Before uploading the images, make sure you create a project named <code>tkg</code>, as described in the previous section, and grant write permissions to <code>&lt;REGISTRY-USERNAME&gt;</code>.</p> <p>If you receive a 500 error from the registry and uploads fail, the registry might be throttling the previous command, as it runs parallel uploads. The troubleshooting section might be helpful.</p> <ol> <li> <p>if you plan to do an air-gapped TKG installation, your bastion host does not have access to the internal registry, therefore you need to copy the images to another host that does, which is tipically inside the isolated environment\u00a0\u21a9</p> </li> </ol>"},{"location":"tkgm/troubleshooting/","title":"Troubleshooting","text":""},{"location":"tkgm/troubleshooting/#change-kube-vip-ip-address","title":"Change Kube-VIP IP address","text":"<p>You should never need to change the IP address of the Kube-VIP load balancer, because it is something you should carefully plan and decide upfront; but in case you do need it, this section may be helpful.</p> <p>Warning</p> <p>This is a quite impactful operation, as it may (and will) break the connections between all the nodes. In the end you might also need to roll all the worker nodes out, for the change to be fully propagated. So, depending on your cluster size, get yourself a maintenance window long enough to rebuild all the nodes.</p> <p>Do remember that you have to pick an address belonging to the same control-plane nodes network, and make sure it is outside the DHCP range.</p> <p>Kube-VIP is managed via a static pod running on the control-plane nodes, this means that its definition lies on a plain YAML file on the node's file system. For kubeadm-based clusters, the static pods manifest files are stored in the <code>/etc/kubernetes/manifests</code> directory, and TKG is no exception.</p> <p>Log into the control-plane nodes and go to the manifests directory. Print the definition of the Kube-VIP pod, if you wish:</p> <pre><code>cd /etc/kubernetes/manifests\ncat kube-vip.yaml\n</code></pre> <p>Now you need to edit the YAML file and set the value of the environment variable named <code>address</code> to the new IP address, like:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nname: kube-vip\nnamespace: kube-system\nspec:\ncontainers:\n- args:\n- manager\nenv:\n...\n- name: address\nvalue: 172.31.8.12\n...\n</code></pre> <p>Do the same thing on all the control-plane nodes of the cluster, and then delete all the <code>kube-vip-*</code> pods from the <code>kube-system</code> namespace, to get them recreated right away with the new configuration:</p> <pre><code>kubectl -n kube-system get pods -o name | grep kube-vip- | xargs kubectl -n kube-system delete\n</code></pre> <p>You also need to change the ConfigMap <code>kubeadm-config</code> in the <code>kube-system</code> namespace, to make sure that future nodes will get configured correctly. Set the key <code>controlPlaneEndpoint</code> in <code>data.ClusterConfiguration</code> to the new IP address, followed by <code>:6443</code> (the standard kube-api port does not change).</p>"},{"location":"tkgm/troubleshooting/#add-names-or-ips-to-api-server-certificate","title":"Add names or IPs to API server certificate","text":"<p>If you want to reach out to your Kubernetes cluster using different DNS names and/or IP addresses (i.e. you need to change the Kube-VIP IP address), you may also want to add those endpoints as SANs in the API server certificate.</p> <p>First of all, you need to log into the control-plane nodes via SSH and <code>cd</code> to the <code>/etc/kubernetes/pki</code> directory. Here you can find all the certificates and keys used by Kubernetes, and the one you care about is <code>apiserver.crt</code>. You can actually take a look at it and make sure it does not include the new endpoints, yet:</p> <pre><code>sudo openssl x509 -text &lt; apiserver.crt\n</code></pre> <p>The <code>kubeadm</code> tool is used for the whole lifecycle of the cluster, and it comes in handy in this case, too. For example, you can check certificates expiration executing</p> <pre><code>kubeadm certs check-expiration\n</code></pre> <p>which shows you also which CA signed the certificates and whether it is external or not.</p> <p>Warning</p> <p>The following procedure applies when no external CA has been used to sign the API server certificate.</p> <p>Kubeadm stores its configuration in a ConfigMap in the cluster, and that's where you start from to add the new endpoints. Fetch the ConfigMap contents and store it to a file:</p> <pre><code>sudo kubectl -n kube-system get cm kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' --kubeconfig /etc/kubernetes/admin.conf &gt; /tmp/kubeadm.conf\n</code></pre> <p>Then, edit the <code>/tmp/kubeadm.conf</code> file to add (or edit if already present) the <code>apiServer.certSANs</code> key, as follows:</p> <pre><code>apiServer:\ncertSANs:\n- 172.31.8.12\n- my.cluster.endpoint.fqdn\n</code></pre> <p>Make sure you do remove the old certificate and key, otherwise <code>kubeadm</code> will not create a new one. Store them somewhere else rather than deleting them, just in case:</p> <pre><code>sudo mv /etc/kubernetes/pki/apiserver.{crt,key} ~\n</code></pre> <p>Now run the command to build a new certificate and key pair:</p> <pre><code>sudo kubeadm init phase certs apiserver --config /tmp/kubeadm.conf\n</code></pre> <p>The API server picks up the new certificate immediately and you can assert it includes the new SANs.</p> <p>Warning</p> <p>If the control-plane is composed of multiple nodes, do make sure you repeat the same process on all of them. DO NOT copy the certificate across all the nodes, as its SANs include also the IP address of the node itself, which is unique for each node. However, you can feed the same <code>kubeadm.conf</code> file to the previous <code>kubeadm</code> command on all nodes.</p> <p>Finally, upload the modified kubeadm configuration back to the ConfigMap in the cluster</p> <pre><code>kubeadm init phase upload-config kubeadm --config /tmp/kubeadm.conf --kubeconfig /etc/kubernetes/admin.conf\n</code></pre>"},{"location":"tkgm/troubleshooting/#change-cluster-configuration","title":"Change cluster configuration","text":"<p>The <code>Cluster</code> objects represent the actual Tanzu clusters known to the management cluster, both the management itself and the workload clusters. A change to those object forces all the nodes of the cluster (control-plane and worker nodes) to be rolled out with the new configuration.</p> <p>For example, if you need to apply or change the proxy settings to the cluster, you can add/change an item, similar to the following, to the <code>spec.topology.variables</code> list:</p> <pre><code>- name: proxy\nvalue:\nhttpProxy: http://proxy.my.domain:proxy-port\nhttpsProxy: http://proxy.my.domain:proxy-port\nnoProxy:\n- .my.domain\n- 10.0.0.0/8\n- 172.16.0.0/12\n- 192.168.0.0/16\n- localhost\n- 127.0.0.1\n- .svc\n- .svc.cluster.local\n- 100.96.0.0/11\n- 100.64.0.0/13\n</code></pre> <p>You can then observe the nodes as they get replaced.</p> <p>Warning</p> <p>Be aware that this will make the connection drop, as the Kube-VIP will need to be re-negotiated between the old and new node(s), so make sure you run this during a proper maintenance window.</p>"},{"location":"tkgm/troubleshooting/#the-registry-throttles-the-upload-bundle-command","title":"The registry throttles the upload-bundle command","text":"<p>The <code>tanzu isolated-cluster upload-bundle</code> command pushes multiple images in parallel to the registry, and this may cause congestion resulting in upload failures.</p> <p>As of now, the above command has no flags or modifiers (that I'm aware of) to decrease the number of parallel uploads, but you can manually push the images leveraging <code>imgpkg</code> directly.</p> <p>The <code>tanzu isolated-cluster download-bundle</code> command, used to pull the images, creates the <code>publish-images-fromtar.yaml</code> file to map every tar file to its correspondent OCI image path, and it looks like</p> <pre><code>ako-operator-v1.7.0_vmware.3.tar: ako-operator\nako-v1.8.2_vmware.1.tar: ako\nantrea-advanced-debian-v1.7.2_vmware.1.tar: antrea-advanced-debian\nazure-cloud-controller-manager-v1.1.26_vmware.1.tar: azure-cloud-controller-manager\nazure-cloud-controller-manager-v1.23.23_vmware.1.tar: azure-cloud-controller-manager\nazure-cloud-controller-manager-v1.24.10_vmware.1.tar: azure-cloud-controller-manager\nazure-cloud-node-manager-v1.1.26_vmware.1.tar: azure-cloud-node-manager\nazure-cloud-node-manager-v1.23.23_vmware.1.tar: azure-cloud-node-manager\nazure-cloud-node-manager-v1.24.10_vmware.1.tar: azure-cloud-node-manager\ncalico-all-cni-v3.24.1_vmware.1.tar: calico-all/cni\n...\n</code></pre> <p>So the following snippet can be used to parse this file and push the images sequentially:</p> <pre><code>cd /path/to/tkgm-bundle\nyq e 'to_entries|map(\"imgpkg copy --tar \" + .key + \" --to-repo ${TKG_CUSTOM_IMAGE_REPOSITORY}/\" + .value + \" --registry-ca-cert-path /path/to/harbor-ca.pem\")|.[]' publish-images-fromtar.yaml | bash\n</code></pre> <p>This script uses the same info as the images relocation section, like the <code>tkgm-bundle</code> directory and the <code>harbor-ca.pem</code> file.</p>"},{"location":"tkgm/workload-cluster/","title":"TKG workload cluster","text":"<p>Info</p> <p>The official VMware documentation is available at https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/2.1/using-tkg-21/workload-index.html.</p> <p>Tanzu Kubernetes Grid workload clusters are the Kubernetes clusters responsible for running your applications, and can be created using the Tanzu CLI from a TKG management cluster on the same platform (i.e. vSphere).</p> <p>You can create three types of workload clusters:</p> <ul> <li>Class-based clusters (default)</li> <li>Plan-based clusters (legacy)</li> <li>TKC-based clusters (legacy)</li> </ul> <p>More information about the different classes are available on VMware docs website. This guide explains how to create class-based clusters.</p>"},{"location":"tkgm/workload-cluster/#prepare-workload-cluster-definition","title":"Prepare workload cluster definition","text":"<p>The class-based workflow cluster manifest can be created from a flat file, similar to the one used for building the management cluster. Following VMware docs, you can create the flat configuration file, for example:</p> <pre><code>AVI_CONTROL_PLANE_HA_PROVIDER: false\nCLUSTER_ANNOTATIONS: 'description:alpha-workload-cluster,location:non,mc:t1xl'\nCLUSTER_CIDR: 100.96.0.0/11\nSERVICE_CIDR: 100.64.0.0/13\nCLUSTER_PLAN: dev\nNAMESPACE: default\nCNI: antrea\nENABLE_AUDIT_LOGGING: false\nENABLE_CEIP_PARTICIPATION: false\nENABLE_MHC: true\nENABLE_MHC_CONTROL_PLANE: true\nENABLE_MHC_WORKER_NODE: true\nMHC_UNKNOWN_STATUS_TIMEOUT: 5m\nMHC_FALSE_STATUS_TIMEOUT: 12m\nIDENTITY_MANAGEMENT_TYPE: none\nINFRASTRUCTURE_PROVIDER: vsphere\nOS_ARCH: amd64\nOS_NAME: photon\nOS_VERSION: \"3\"\nTKG_HTTP_PROXY_ENABLED: false\nVSPHERE_CONTROL_PLANE_ENDPOINT: my.endpoint.fqdn.or.ip\nVSPHERE_DATACENTER: /vc01\nVSPHERE_DATASTORE: /vc01/datastore/vsanDatastore\nVSPHERE_FOLDER: /vc01/vm/tkgm/alpha\nVSPHERE_INSECURE: false\nVSPHERE_NETWORK: /vc01/network/tkgm_vip_workload\nVSPHERE_PASSWORD: &lt;encoded:bXktb3duLXBhc3N3b3JkCg==&gt;\nVSPHERE_RESOURCE_POOL: /vc01/host/vc01cl01/Resources\nVSPHERE_SERVER: my.vcenter.fqdn.or.ip\nVSPHERE_SSH_AUTHORIZED_KEY: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICAKfCTddh2+U/Y3DmhTwq9pqr+bWCZFxEOF1S2LVxsR\nVSPHERE_TLS_THUMBPRINT: B6:C1:C2:77:2F:E5:BA:C7:C1:C3:AD:59:E1:76:E4:19:13:75:30:ED\nVSPHERE_USERNAME: administrator@vsphere.local\n</code></pre> <p>You can also use a different user with fewer privileges than <code>administrator@vsphere.local</code> for building the workload cluster, more info on this will be provided in a different section. All settings are detailed in the VMware docs website.</p> <p>You can get the vCenter SHA1 fingerprint (for the <code>VSPHERE_TLS_THUMBPRINT</code> setting) as</p> <pre><code>openssl s_client -connect my.vcenter.fqdn.or.ip:443 -servername my.vcenter.fqdn.or.ip -showcerts &lt;/dev/null 2&gt;/dev/null | openssl x509 -fingerprint -noout -sha1 | cut -d= -f2\n</code></pre> <p>Warning</p> <p>The <code>-sha1</code> flag is necessary especially on MacOS's openssl, where the default output digest is md5.</p> <p>You may now convert the flat file to the class-based file. First of all, set the Tanzu CLI to not apply automatically the generated class-based file:</p> <pre><code>tanzu config set features.cluster.auto-apply-generated-clusterclass-based-configuration false\n</code></pre> <p>then you can produce such file as</p> <pre><code>tanzu cluster create alpha --file alpha-config.yaml --dry-run &gt; alpha-spec.yaml\n</code></pre> <p>This command takes also care of validating your inputs, therefore if something is not right (i.e. the designated vSphere folder does not exist) it will fail with a meaningful message.</p> <p>The generated YAML file defines these resource types:</p> <ul> <li><code>VSphereCPIConfig</code></li> <li><code>VSphereCSIConfig</code></li> <li><code>ClusterBootstrap</code></li> <li><code>Secret</code></li> <li><code>Cluster</code></li> </ul> <p>The <code>ClusterBootstrap</code> defines which components will be deployed to the new cluster as Tanzu packages. One thing to notice is that those components do not include the version, but they rather specify <code>*</code></p> <pre><code>apiVersion: run.tanzu.vmware.com/v1alpha3\nkind: ClusterBootstrap\nmetadata:\nannotations:\ntkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.24.9---vmware.1-tkg.1\nname: alpha\nnamespace: default\nspec:\nadditionalPackages:\n- refName: metrics-server*\n- refName: secretgen-controller*\n- refName: pinniped*\ncpi:\nrefName: vsphere-cpi*\nvaluesFrom:\nproviderRef:\napiGroup: cpi.tanzu.vmware.com\nkind: VSphereCPIConfig\nname: alpha\ncsi:\nrefName: vsphere-csi*\nvaluesFrom:\nproviderRef:\napiGroup: csi.tanzu.vmware.com\nkind: VSphereCSIConfig\nname: alpha\nkapp:\nrefName: kapp-controller*\n</code></pre> <p>to include any possible version that may have been made available to the cluster via a Tanzu package repository.</p> <p>The downside is that this resource cannot be used as a desired state and thus included into a GitOps-like process, as those <code>*</code>s are immediately translated into actual versions as soon as the workload cluster gets created.</p>"},{"location":"tkgm/workload-cluster/#create-workload-cluster","title":"Create workload cluster","text":"<p>You now have the YAML manifest for the new cluster, which must be applied to the management cluster.</p> <p>Make sure you do have your management cluster configuration and set it as your default kubeconfig file. You should be able to find it at <code>~/.kube-tkg/config</code>, so you just have to set</p> <pre><code>export KUBECONFIG=~/.kube-tkg/config\n</code></pre> <p>and you can check you're connected to the right endpoint via</p> <pre><code>kubectl get nodes\n</code></pre> <p>You can also retrieve the kubeconfig file via</p> <pre><code>tanzu mc kubeconfig get --admin\n</code></pre> <p>and switch to the right context. Now apply the workload cluster manifest:</p> <pre><code>kubectl apply -f alpha-spec.yaml\n</code></pre> <p>You can monitor the status of the resources running a <code>kubectl get</code> wrapped in a <code>watch</code>:</p> <pre><code>watch kubectl get md,ma,vspherevm,vspheremachines\n</code></pre> <p>and</p> <pre><code>\u276f tanzu cluster list\n\n  NAME   NAMESPACE  STATUS   CONTROLPLANE  WORKERS  KUBERNETES        ROLES   PLAN  TKR\n  alpha  default    running  1/1           2/2      v1.24.9+vmware.1  &lt;none&gt;  dev   v1.24.9---vmware.1-tkg.1\n</code></pre>"},{"location":"tkgm/workload-cluster/#connect-to-the-new-cluster","title":"Connect to the new cluster","text":"<p>Ask the Tanzu CLI to fetch the kubeconfig file for you and store it to a file. To get the configuration added to the currently active kubeconfig file you must omit the <code>--export-file</code> flag:</p> <pre><code>\u276f tanzu cluster kubeconfig get alpha --admin --export-file alpha.kubeconfig\n\nCredentials of cluster 'alpha' have been saved\nYou can now access the cluster by running 'kubectl config use-context alpha-admin@alpha' under path 'alpha.kubeconfig'\n</code></pre> <p>Check the nodes status</p> <pre><code>\u276f export KUBECONFIG=$(pwd)/alpha.kubeconfig\n\u276f kubectl get nodes\nNAME                                STATUS   ROLES           AGE     VERSION\nalpha-md-0-9qpjk-8694659b79-7mpxs   Ready    &lt;none&gt;          91s     v1.24.9+vmware.1\nalpha-md-0-9qpjk-8694659b79-vfrbk   Ready    &lt;none&gt;          89s     v1.24.9+vmware.1\nalpha-rv6pp-fkqg4                   Ready    control-plane   2m28s   v1.24.9+vmware.1\n</code></pre> <p>You can now test the cluster with a sample nginx web server. As this environment is isolated, you need to push the image to the internal registry first (this step might be documented elsewhere in the future), and then you can run it. In this example, I created an <code>apps</code> project on the registry and pushed a <code>nginx:stable-alpine</code> to it.</p> <pre><code>kubectl run --image registry.my.domain/apps/nginx:stable-alpine nginx\nkubectl port-forward pods/nginx 8080:80\n</code></pre> <p>and, in another terminal,</p> <pre><code>curl localhost:8080\n</code></pre> <p>the response confirms the pod is running correctly.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"tkgm/workload-cluster/#configure-tanzu-standard-repository","title":"Configure Tanzu standard repository","text":"<p>TKG provides a number of packages available to be installed and used right out of the box, based on the Carvel kapp packaging system. They are delivered as part of a kapp repository, which is also available as OCI image, thats has been relocated in the internal registry during image relocation.</p> <p>You can add the repository as</p> <pre><code>tanzu package repository add tanzu-standard --url ${TKG_CUSTOM_IMAGE_REPOSITORY}/packages/standard/repo:v2.1.0 --namespace tkg-system\n</code></pre> <p>and, as soon as the reconciliation succeeds, you can list available packages:</p> <pre><code>\u276f tanzu package available list\n\n  NAME                                          DISPLAY-NAME\n  cert-manager.tanzu.vmware.com                 cert-manager\n  contour.tanzu.vmware.com                      contour\n  external-dns.tanzu.vmware.com                 external-dns\n  fluent-bit.tanzu.vmware.com                   fluent-bit\n  fluxcd-helm-controller.tanzu.vmware.com       Flux Helm Controller\n  fluxcd-kustomize-controller.tanzu.vmware.com  Flux Kustomize Controller\n  fluxcd-source-controller.tanzu.vmware.com     Flux Source Controller\n  grafana.tanzu.vmware.com                      grafana\n  harbor.tanzu.vmware.com                       harbor\n  multus-cni.tanzu.vmware.com                   multus-cni\n  prometheus.tanzu.vmware.com                   prometheus\n  whereabouts.tanzu.vmware.com                  whereabouts\n</code></pre> <p>The packages are available in all namespaces, as the repository has been installed in kapp-controller's packaging global namespace.</p>"}]}